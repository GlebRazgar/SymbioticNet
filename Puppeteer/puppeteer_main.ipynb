{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"65670791be0f9dbdabacec767b7aa930f88f57cb"},"source":["#### Imports"]},{"cell_type":"code","execution_count":20,"metadata":{"_uuid":"8b92df54d984e6e40ada9af22e0212e435dbf783","trusted":true},"outputs":[],"source":["#pytorch utility imports\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision.utils import make_grid\n","\n","#neural net imports\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","#import external libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import os\n","import math\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"_uuid":"a37121ac5ef70c0cb0899b0d8d996c40b450c826"},"source":["Check for CUDA"]},{"cell_type":"code","execution_count":21,"metadata":{"_uuid":"cc31bfe822c48ef3a7911eb3a3f62e4116f6befc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n","cuda\n"]}],"source":["print(torch.cuda.is_available())\n","print(torch.backends.cudnn.enabled)\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","print(device)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sample_submission.csv  test.csv  train.csv\n"]}],"source":["! ls ../Data/\n","Data_folder_path = \"../Data/\"\n","train_df = pd.read_csv(Data_folder_path+\"train.csv\")\n","test_df = pd.read_csv(Data_folder_path+\"test.csv\")"]},{"cell_type":"markdown","metadata":{"_uuid":"d9dd9d5effcb7f16090ce11cfcd748ea7fc651e5"},"source":["#### Separate into labels and training images and reshape the images"]},{"cell_type":"code","execution_count":23,"metadata":{"_uuid":"ceb1cc8b91f268b7864b4ce8a135ee0f36863b4d","trusted":true},"outputs":[],"source":["train_labels = train_df['label'].values\n","train_images = (train_df.iloc[:,1:].values).astype('float32')\n","test_images = (test_df.iloc[:,:].values).astype('float32')\n","\n","#Training and Validation Split\n","train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n","                                                                     stratify=train_labels, random_state=123,\n","                                                                     test_size=0.20)"]},{"cell_type":"code","execution_count":24,"metadata":{"_uuid":"a0467a07f79074cb03dfafcf70a989a1de654e8e","trusted":true},"outputs":[],"source":["train_images = train_images.reshape(train_images.shape[0], 28, 28)\n","val_images = val_images.reshape(val_images.shape[0], 28, 28)\n","test_images = test_images.reshape(test_images.shape[0], 28, 28)"]},{"cell_type":"markdown","metadata":{"_uuid":"f5cc49c6ced3bbacfc4f2dbdfab19471ec4ddb04"},"source":["#### Plot some images to see samples"]},{"cell_type":"code","execution_count":25,"metadata":{"_uuid":"1de567ffcf179220917f3344066454f32a6127bb","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 3 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#train samples\n","for i in range(6, 9):\n","    plt.subplot(330 + (i+1))\n","    plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n","    plt.title(train_labels[i])"]},{"cell_type":"code","execution_count":26,"metadata":{"_uuid":"1ebccd63893509f0bddea4d0d6f783206796edf5","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 3 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#test samples\n","for i in range(6, 9):\n","    plt.subplot(330 + (i+1))\n","    plt.imshow(test_images[i].squeeze(), cmap=plt.get_cmap('gray'))"]},{"cell_type":"markdown","metadata":{"_uuid":"fa81fe0a91f7894e144d09b9268d67a6773c51b6"},"source":["#### Convert images to tensors\n","Normalize the images too"]},{"cell_type":"code","execution_count":27,"metadata":{"_uuid":"d39d3fb5b0613fabf72d4a17ddf638bf4ad28700","trusted":true},"outputs":[],"source":["#train\n","train_images_tensor = torch.tensor(train_images)/255.0\n","train_labels_tensor = torch.tensor(train_labels)\n","train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n","\n","#val\n","val_images_tensor = torch.tensor(val_images)/255.0\n","val_labels_tensor = torch.tensor(val_labels)\n","val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n","\n","#test\n","test_images_tensor = torch.tensor(test_images)/255.0"]},{"cell_type":"markdown","metadata":{"_uuid":"eea027734384efcc2ff337c0211d2685d95989b0"},"source":["#### Load images into the data generator"]},{"cell_type":"code","execution_count":28,"metadata":{"_uuid":"62de8a26a44cafbf5274ee914142fb56a58b25d4","trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_tensor, batch_size=8, num_workers=0, shuffle=True)\n","val_loader = DataLoader(val_tensor, batch_size=16, num_workers=0, shuffle=True)\n","test_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=0, shuffle=False)"]},{"cell_type":"markdown","metadata":{"_uuid":"7b72d3e520cb453ffa0ebc65d4809ccf7bb1aadd"},"source":["#### Plot some sample images using the data generator"]},{"cell_type":"code","execution_count":29,"metadata":{"_uuid":"ff324b9c832ff8959aa84902ccf9403f9483a879","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/miniconda3/envs/ef_env/lib/python3.6/site-packages/matplotlib/text.py:1163: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  if s != self._text:\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["for batch_idx, (data, target) in enumerate(train_loader):\n","    img_grid = make_grid(data[0:8,].unsqueeze(1), nrow=8)\n","    img_target_labels = target[0:8,].numpy()\n","    break\n","    \n","plt.imshow(img_grid.numpy().transpose((1,2,0)))\n","plt.rcParams['figure.figsize'] = (10, 2)\n","plt.title(img_target_labels, size=16)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"f99f8c9b1bfe0474f27b72005e7708b6d9fd091a","trusted":true},"source":["#### Define the CNN Model"]},{"cell_type":"code","execution_count":109,"metadata":{"_uuid":"16954f2ed80f6ecceff5b48e7566415857432a76","trusted":true},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        # Convolutional Layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        \n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        \n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        \n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        # Fully Connected Layers\n","        self.dropout1 = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(128*7*7, 128)\n","        self.bn_fc1 = nn.BatchNorm1d(128)\n","        \n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.bn_fc2 = nn.BatchNorm1d(64)\n","        \n","        self.dropout3 = nn.Dropout(0.5)\n","        self.fc3 = nn.Linear(64, 10)\n","        \n","    def forward(self, x, adjustments=None):\n","        # First Convolutional Layer\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        if adjustments is not None and 'conv1' in adjustments:\n","            x = x + adjustments['conv1']\n","        \n","        # Second Convolutional Layer\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        if adjustments is not None and 'conv2' in adjustments:\n","            x = x + adjustments['conv2']\n","        \n","        x = self.pool1(x)\n","        \n","        # Third Convolutional Layer\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = F.relu(x)\n","        if adjustments is not None and 'conv3' in adjustments:\n","            x = x + adjustments['conv3']\n","        \n","        x = self.pool2(x)\n","        \n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","        \n","        # First Fully Connected Layer\n","        x = self.dropout1(x)\n","        x = self.fc1(x)\n","        x = self.bn_fc1(x)\n","        x = F.relu(x)\n","        if adjustments is not None and 'fc1' in adjustments:\n","            x = x + adjustments['fc1']\n","        \n","        # Second Fully Connected Layer\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        x = self.bn_fc2(x)\n","        x = F.relu(x)\n","        if adjustments is not None and 'fc2' in adjustments:\n","            x = x + adjustments['fc2']\n","        \n","        # Output Layer (Do not adjust)\n","        x = self.dropout3(x)\n","        x = self.fc3(x)\n","        \n","        return x\n","\n","# Instantiate the model\n","conv_model = Net()\n"]},{"cell_type":"markdown","metadata":{"_uuid":"8c78e04e0466bb60c949f78e0ebb68e1bf5f34a5","trusted":true},"source":["#### Define the optimizer and loss functions"]},{"cell_type":"code","execution_count":110,"metadata":{"_uuid":"1ea02395180c3997298a4bae8f21f8a26aafb5c4","trusted":true},"outputs":[],"source":["optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n","criterion = nn.CrossEntropyLoss()\n","\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","    criterion = criterion.cuda()\n"]},{"cell_type":"markdown","metadata":{"_uuid":"ead9edefdda79f314421cefe8b174eb22f3502a5","trusted":true},"source":["#### Training the Model"]},{"cell_type":"code","execution_count":111,"metadata":{"_uuid":"7a6d0608892e522c76c7616dcd67c740b792f528","trusted":true},"outputs":[],"source":["def train_model(num_epoch):\n","    conv_model.train()\n","    exp_lr_scheduler.step()\n","    \n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data = data.unsqueeze(1)\n","        \n","        if torch.cuda.is_available():\n","            data = data.cuda()\n","            target = target.cuda()\n","                \n","        optimizer.zero_grad()\n","        output = conv_model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (batch_idx + 1)% 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                num_epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n","                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n","                \n","def evaluate(data_loader):\n","    conv_model.eval()\n","    loss = 0\n","    correct = 0\n","    \n","    with torch.no_grad():\n","        for data, target in data_loader:\n","            data = data.unsqueeze(1)\n","            \n","            if torch.cuda.is_available():\n","                data = data.cuda()\n","                target = target.cuda()\n","            \n","            output = conv_model(data)\n","            loss += criterion(output, target).item() * data.size(0)\n","            pred = output.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","    loss /= len(data_loader.dataset)\n","        \n","    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n","        loss, correct, len(data_loader.dataset),\n","        100. * correct / len(data_loader.dataset)))\n"]},{"cell_type":"code","execution_count":112,"metadata":{"_uuid":"aa9e16954b11b99d018779f3322285a6c6ddfaf2","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 0 [800/33600 (2%)]\tLoss: 1.589806\n","Train Epoch: 0 [1600/33600 (5%)]\tLoss: 1.668469\n","Train Epoch: 0 [2400/33600 (7%)]\tLoss: 0.604120\n","Train Epoch: 0 [3200/33600 (10%)]\tLoss: 1.337416\n","Train Epoch: 0 [4000/33600 (12%)]\tLoss: 0.230074\n","Train Epoch: 0 [4800/33600 (14%)]\tLoss: 0.423348\n","Train Epoch: 0 [5600/33600 (17%)]\tLoss: 0.443601\n","Train Epoch: 0 [6400/33600 (19%)]\tLoss: 0.272487\n","Train Epoch: 0 [7200/33600 (21%)]\tLoss: 0.141826\n","Train Epoch: 0 [8000/33600 (24%)]\tLoss: 0.645023\n","Train Epoch: 0 [8800/33600 (26%)]\tLoss: 0.534558\n","Train Epoch: 0 [9600/33600 (29%)]\tLoss: 0.495124\n","Train Epoch: 0 [10400/33600 (31%)]\tLoss: 0.659189\n","Train Epoch: 0 [11200/33600 (33%)]\tLoss: 0.542258\n","Train Epoch: 0 [12000/33600 (36%)]\tLoss: 0.343066\n","Train Epoch: 0 [12800/33600 (38%)]\tLoss: 0.873143\n","Train Epoch: 0 [13600/33600 (40%)]\tLoss: 0.137561\n","Train Epoch: 0 [14400/33600 (43%)]\tLoss: 0.047371\n","Train Epoch: 0 [15200/33600 (45%)]\tLoss: 0.989732\n","Train Epoch: 0 [16000/33600 (48%)]\tLoss: 0.309987\n","Train Epoch: 0 [16800/33600 (50%)]\tLoss: 0.406326\n","Train Epoch: 0 [17600/33600 (52%)]\tLoss: 0.324932\n","Train Epoch: 0 [18400/33600 (55%)]\tLoss: 0.604695\n","Train Epoch: 0 [19200/33600 (57%)]\tLoss: 0.141067\n","Train Epoch: 0 [20000/33600 (60%)]\tLoss: 0.135551\n","Train Epoch: 0 [20800/33600 (62%)]\tLoss: 0.078678\n","Train Epoch: 0 [21600/33600 (64%)]\tLoss: 0.496283\n","Train Epoch: 0 [22400/33600 (67%)]\tLoss: 0.474590\n","Train Epoch: 0 [23200/33600 (69%)]\tLoss: 0.517987\n","Train Epoch: 0 [24000/33600 (71%)]\tLoss: 0.119967\n","Train Epoch: 0 [24800/33600 (74%)]\tLoss: 0.128190\n","Train Epoch: 0 [25600/33600 (76%)]\tLoss: 0.236870\n","Train Epoch: 0 [26400/33600 (79%)]\tLoss: 0.144994\n","Train Epoch: 0 [27200/33600 (81%)]\tLoss: 0.219357\n","Train Epoch: 0 [28000/33600 (83%)]\tLoss: 0.487428\n","Train Epoch: 0 [28800/33600 (86%)]\tLoss: 0.235138\n","Train Epoch: 0 [29600/33600 (88%)]\tLoss: 0.307219\n","Train Epoch: 0 [30400/33600 (90%)]\tLoss: 1.012926\n","Train Epoch: 0 [31200/33600 (93%)]\tLoss: 0.179537\n","Train Epoch: 0 [32000/33600 (95%)]\tLoss: 0.314371\n","Train Epoch: 0 [32800/33600 (98%)]\tLoss: 0.471875\n","Train Epoch: 0 [33600/33600 (100%)]\tLoss: 0.064458\n","\n","Average Val Loss: 0.0668, Val Accuracy: 8254/8400 (98.262%)\n","\n","Train Epoch: 1 [800/33600 (2%)]\tLoss: 1.045924\n","Train Epoch: 1 [1600/33600 (5%)]\tLoss: 0.269964\n","Train Epoch: 1 [2400/33600 (7%)]\tLoss: 0.081194\n","Train Epoch: 1 [3200/33600 (10%)]\tLoss: 0.162201\n","Train Epoch: 1 [4000/33600 (12%)]\tLoss: 0.070750\n","Train Epoch: 1 [4800/33600 (14%)]\tLoss: 0.174920\n","Train Epoch: 1 [5600/33600 (17%)]\tLoss: 0.123342\n","Train Epoch: 1 [6400/33600 (19%)]\tLoss: 0.037563\n","Train Epoch: 1 [7200/33600 (21%)]\tLoss: 0.534598\n","Train Epoch: 1 [8000/33600 (24%)]\tLoss: 0.511965\n","Train Epoch: 1 [8800/33600 (26%)]\tLoss: 0.542972\n","Train Epoch: 1 [9600/33600 (29%)]\tLoss: 2.168326\n","Train Epoch: 1 [10400/33600 (31%)]\tLoss: 1.025447\n","Train Epoch: 1 [11200/33600 (33%)]\tLoss: 0.162555\n","Train Epoch: 1 [12000/33600 (36%)]\tLoss: 0.068336\n","Train Epoch: 1 [12800/33600 (38%)]\tLoss: 0.033103\n","Train Epoch: 1 [13600/33600 (40%)]\tLoss: 0.498188\n","Train Epoch: 1 [14400/33600 (43%)]\tLoss: 0.133519\n","Train Epoch: 1 [15200/33600 (45%)]\tLoss: 0.191992\n","Train Epoch: 1 [16000/33600 (48%)]\tLoss: 0.071571\n","Train Epoch: 1 [16800/33600 (50%)]\tLoss: 0.062326\n","Train Epoch: 1 [17600/33600 (52%)]\tLoss: 0.243953\n","Train Epoch: 1 [18400/33600 (55%)]\tLoss: 1.578454\n","Train Epoch: 1 [19200/33600 (57%)]\tLoss: 0.219838\n","Train Epoch: 1 [20000/33600 (60%)]\tLoss: 1.115857\n","Train Epoch: 1 [20800/33600 (62%)]\tLoss: 0.056316\n","Train Epoch: 1 [21600/33600 (64%)]\tLoss: 0.016922\n","Train Epoch: 1 [22400/33600 (67%)]\tLoss: 0.448566\n","Train Epoch: 1 [23200/33600 (69%)]\tLoss: 0.386101\n","Train Epoch: 1 [24000/33600 (71%)]\tLoss: 0.115757\n","Train Epoch: 1 [24800/33600 (74%)]\tLoss: 0.119534\n","Train Epoch: 1 [25600/33600 (76%)]\tLoss: 0.052918\n","Train Epoch: 1 [26400/33600 (79%)]\tLoss: 0.594282\n","Train Epoch: 1 [27200/33600 (81%)]\tLoss: 0.007210\n","Train Epoch: 1 [28000/33600 (83%)]\tLoss: 1.677718\n","Train Epoch: 1 [28800/33600 (86%)]\tLoss: 0.030747\n","Train Epoch: 1 [29600/33600 (88%)]\tLoss: 0.263455\n","Train Epoch: 1 [30400/33600 (90%)]\tLoss: 0.193678\n","Train Epoch: 1 [31200/33600 (93%)]\tLoss: 0.525554\n","Train Epoch: 1 [32000/33600 (95%)]\tLoss: 0.101919\n","Train Epoch: 1 [32800/33600 (98%)]\tLoss: 0.280977\n","Train Epoch: 1 [33600/33600 (100%)]\tLoss: 0.075591\n","\n","Average Val Loss: 0.0550, Val Accuracy: 8267/8400 (98.417%)\n","\n","Train Epoch: 2 [800/33600 (2%)]\tLoss: 0.087491\n","Train Epoch: 2 [1600/33600 (5%)]\tLoss: 0.168001\n","Train Epoch: 2 [2400/33600 (7%)]\tLoss: 0.135103\n","Train Epoch: 2 [3200/33600 (10%)]\tLoss: 0.114561\n","Train Epoch: 2 [4000/33600 (12%)]\tLoss: 0.003483\n","Train Epoch: 2 [4800/33600 (14%)]\tLoss: 0.126749\n","Train Epoch: 2 [5600/33600 (17%)]\tLoss: 0.355044\n","Train Epoch: 2 [6400/33600 (19%)]\tLoss: 1.051887\n","Train Epoch: 2 [7200/33600 (21%)]\tLoss: 0.088019\n","Train Epoch: 2 [8000/33600 (24%)]\tLoss: 0.117801\n","Train Epoch: 2 [8800/33600 (26%)]\tLoss: 0.019361\n","Train Epoch: 2 [9600/33600 (29%)]\tLoss: 0.044717\n","Train Epoch: 2 [10400/33600 (31%)]\tLoss: 0.200997\n","Train Epoch: 2 [11200/33600 (33%)]\tLoss: 0.070010\n","Train Epoch: 2 [12000/33600 (36%)]\tLoss: 0.501942\n","Train Epoch: 2 [12800/33600 (38%)]\tLoss: 0.159154\n","Train Epoch: 2 [13600/33600 (40%)]\tLoss: 0.029416\n","Train Epoch: 2 [14400/33600 (43%)]\tLoss: 0.928960\n","Train Epoch: 2 [15200/33600 (45%)]\tLoss: 0.097527\n","Train Epoch: 2 [16000/33600 (48%)]\tLoss: 0.787886\n","Train Epoch: 2 [16800/33600 (50%)]\tLoss: 0.186166\n","Train Epoch: 2 [17600/33600 (52%)]\tLoss: 0.270320\n","Train Epoch: 2 [18400/33600 (55%)]\tLoss: 0.545542\n","Train Epoch: 2 [19200/33600 (57%)]\tLoss: 0.430023\n","Train Epoch: 2 [20000/33600 (60%)]\tLoss: 0.084206\n","Train Epoch: 2 [20800/33600 (62%)]\tLoss: 1.828511\n","Train Epoch: 2 [21600/33600 (64%)]\tLoss: 0.399293\n","Train Epoch: 2 [22400/33600 (67%)]\tLoss: 0.211842\n","Train Epoch: 2 [23200/33600 (69%)]\tLoss: 0.125351\n","Train Epoch: 2 [24000/33600 (71%)]\tLoss: 0.417384\n","Train Epoch: 2 [24800/33600 (74%)]\tLoss: 0.452484\n","Train Epoch: 2 [25600/33600 (76%)]\tLoss: 0.063657\n","Train Epoch: 2 [26400/33600 (79%)]\tLoss: 0.038626\n","Train Epoch: 2 [27200/33600 (81%)]\tLoss: 0.020698\n","Train Epoch: 2 [28000/33600 (83%)]\tLoss: 0.135629\n","Train Epoch: 2 [28800/33600 (86%)]\tLoss: 0.125638\n","Train Epoch: 2 [29600/33600 (88%)]\tLoss: 0.438071\n","Train Epoch: 2 [30400/33600 (90%)]\tLoss: 0.010197\n","Train Epoch: 2 [31200/33600 (93%)]\tLoss: 0.050841\n","Train Epoch: 2 [32000/33600 (95%)]\tLoss: 0.246299\n","Train Epoch: 2 [32800/33600 (98%)]\tLoss: 0.090396\n","Train Epoch: 2 [33600/33600 (100%)]\tLoss: 0.905041\n","\n","Average Val Loss: 0.0512, Val Accuracy: 8283/8400 (98.607%)\n","\n","Train Epoch: 3 [800/33600 (2%)]\tLoss: 0.028347\n","Train Epoch: 3 [1600/33600 (5%)]\tLoss: 0.293589\n","Train Epoch: 3 [2400/33600 (7%)]\tLoss: 0.101848\n","Train Epoch: 3 [3200/33600 (10%)]\tLoss: 0.182141\n","Train Epoch: 3 [4000/33600 (12%)]\tLoss: 0.486857\n","Train Epoch: 3 [4800/33600 (14%)]\tLoss: 0.027815\n","Train Epoch: 3 [5600/33600 (17%)]\tLoss: 0.010479\n","Train Epoch: 3 [6400/33600 (19%)]\tLoss: 0.326391\n","Train Epoch: 3 [7200/33600 (21%)]\tLoss: 0.201381\n","Train Epoch: 3 [8000/33600 (24%)]\tLoss: 0.083641\n","Train Epoch: 3 [8800/33600 (26%)]\tLoss: 0.178143\n","Train Epoch: 3 [9600/33600 (29%)]\tLoss: 0.025133\n","Train Epoch: 3 [10400/33600 (31%)]\tLoss: 1.304798\n","Train Epoch: 3 [11200/33600 (33%)]\tLoss: 0.147974\n","Train Epoch: 3 [12000/33600 (36%)]\tLoss: 0.161819\n","Train Epoch: 3 [12800/33600 (38%)]\tLoss: 0.078005\n","Train Epoch: 3 [13600/33600 (40%)]\tLoss: 0.451929\n","Train Epoch: 3 [14400/33600 (43%)]\tLoss: 0.005864\n","Train Epoch: 3 [15200/33600 (45%)]\tLoss: 0.570358\n","Train Epoch: 3 [16000/33600 (48%)]\tLoss: 0.024916\n","Train Epoch: 3 [16800/33600 (50%)]\tLoss: 0.105024\n","Train Epoch: 3 [17600/33600 (52%)]\tLoss: 0.454680\n","Train Epoch: 3 [18400/33600 (55%)]\tLoss: 0.037453\n","Train Epoch: 3 [19200/33600 (57%)]\tLoss: 0.858793\n","Train Epoch: 3 [20000/33600 (60%)]\tLoss: 0.045735\n","Train Epoch: 3 [20800/33600 (62%)]\tLoss: 0.082099\n","Train Epoch: 3 [21600/33600 (64%)]\tLoss: 0.008067\n","Train Epoch: 3 [22400/33600 (67%)]\tLoss: 0.131121\n","Train Epoch: 3 [23200/33600 (69%)]\tLoss: 0.078377\n","Train Epoch: 3 [24000/33600 (71%)]\tLoss: 0.059953\n","Train Epoch: 3 [24800/33600 (74%)]\tLoss: 0.255151\n","Train Epoch: 3 [25600/33600 (76%)]\tLoss: 0.025620\n","Train Epoch: 3 [26400/33600 (79%)]\tLoss: 0.124649\n","Train Epoch: 3 [27200/33600 (81%)]\tLoss: 0.000895\n","Train Epoch: 3 [28000/33600 (83%)]\tLoss: 0.455041\n","Train Epoch: 3 [28800/33600 (86%)]\tLoss: 0.211017\n","Train Epoch: 3 [29600/33600 (88%)]\tLoss: 0.126072\n","Train Epoch: 3 [30400/33600 (90%)]\tLoss: 0.035649\n","Train Epoch: 3 [31200/33600 (93%)]\tLoss: 0.061497\n","Train Epoch: 3 [32000/33600 (95%)]\tLoss: 0.114268\n","Train Epoch: 3 [32800/33600 (98%)]\tLoss: 0.095930\n","Train Epoch: 3 [33600/33600 (100%)]\tLoss: 0.387955\n","\n","Average Val Loss: 0.0451, Val Accuracy: 8282/8400 (98.595%)\n","\n","Train Epoch: 4 [800/33600 (2%)]\tLoss: 0.144863\n","Train Epoch: 4 [1600/33600 (5%)]\tLoss: 0.180413\n","Train Epoch: 4 [2400/33600 (7%)]\tLoss: 0.327183\n","Train Epoch: 4 [3200/33600 (10%)]\tLoss: 0.343107\n","Train Epoch: 4 [4000/33600 (12%)]\tLoss: 0.004877\n","Train Epoch: 4 [4800/33600 (14%)]\tLoss: 0.236001\n","Train Epoch: 4 [5600/33600 (17%)]\tLoss: 0.023537\n","Train Epoch: 4 [6400/33600 (19%)]\tLoss: 0.188764\n","Train Epoch: 4 [7200/33600 (21%)]\tLoss: 0.139543\n","Train Epoch: 4 [8000/33600 (24%)]\tLoss: 0.148443\n","Train Epoch: 4 [8800/33600 (26%)]\tLoss: 0.300541\n","Train Epoch: 4 [9600/33600 (29%)]\tLoss: 0.087357\n","Train Epoch: 4 [10400/33600 (31%)]\tLoss: 0.004005\n","Train Epoch: 4 [11200/33600 (33%)]\tLoss: 0.123037\n","Train Epoch: 4 [12000/33600 (36%)]\tLoss: 0.285311\n","Train Epoch: 4 [12800/33600 (38%)]\tLoss: 0.031644\n","Train Epoch: 4 [13600/33600 (40%)]\tLoss: 0.058672\n","Train Epoch: 4 [14400/33600 (43%)]\tLoss: 0.292204\n","Train Epoch: 4 [15200/33600 (45%)]\tLoss: 0.157276\n","Train Epoch: 4 [16000/33600 (48%)]\tLoss: 0.398111\n","Train Epoch: 4 [16800/33600 (50%)]\tLoss: 0.098927\n","Train Epoch: 4 [17600/33600 (52%)]\tLoss: 0.478620\n","Train Epoch: 4 [18400/33600 (55%)]\tLoss: 0.774467\n","Train Epoch: 4 [19200/33600 (57%)]\tLoss: 1.401230\n","Train Epoch: 4 [20000/33600 (60%)]\tLoss: 0.098575\n","Train Epoch: 4 [20800/33600 (62%)]\tLoss: 0.087209\n","Train Epoch: 4 [21600/33600 (64%)]\tLoss: 0.659550\n","Train Epoch: 4 [22400/33600 (67%)]\tLoss: 0.512965\n","Train Epoch: 4 [23200/33600 (69%)]\tLoss: 0.014259\n","Train Epoch: 4 [24000/33600 (71%)]\tLoss: 0.160197\n","Train Epoch: 4 [24800/33600 (74%)]\tLoss: 0.049343\n","Train Epoch: 4 [25600/33600 (76%)]\tLoss: 1.607008\n","Train Epoch: 4 [26400/33600 (79%)]\tLoss: 0.493280\n","Train Epoch: 4 [27200/33600 (81%)]\tLoss: 0.180047\n","Train Epoch: 4 [28000/33600 (83%)]\tLoss: 0.236792\n","Train Epoch: 4 [28800/33600 (86%)]\tLoss: 0.352937\n","Train Epoch: 4 [29600/33600 (88%)]\tLoss: 0.026321\n","Train Epoch: 4 [30400/33600 (90%)]\tLoss: 0.597786\n","Train Epoch: 4 [31200/33600 (93%)]\tLoss: 0.997320\n","Train Epoch: 4 [32000/33600 (95%)]\tLoss: 0.023021\n","Train Epoch: 4 [32800/33600 (98%)]\tLoss: 0.016317\n","Train Epoch: 4 [33600/33600 (100%)]\tLoss: 0.087618\n","\n","Average Val Loss: 0.0434, Val Accuracy: 8297/8400 (98.774%)\n","\n","Train Epoch: 5 [800/33600 (2%)]\tLoss: 0.094588\n","Train Epoch: 5 [1600/33600 (5%)]\tLoss: 0.173377\n","Train Epoch: 5 [2400/33600 (7%)]\tLoss: 0.259767\n","Train Epoch: 5 [3200/33600 (10%)]\tLoss: 0.006907\n","Train Epoch: 5 [4000/33600 (12%)]\tLoss: 0.074352\n","Train Epoch: 5 [4800/33600 (14%)]\tLoss: 0.108162\n","Train Epoch: 5 [5600/33600 (17%)]\tLoss: 0.030735\n","Train Epoch: 5 [6400/33600 (19%)]\tLoss: 0.324083\n","Train Epoch: 5 [7200/33600 (21%)]\tLoss: 0.176249\n","Train Epoch: 5 [8000/33600 (24%)]\tLoss: 0.009490\n","Train Epoch: 5 [8800/33600 (26%)]\tLoss: 0.149538\n","Train Epoch: 5 [9600/33600 (29%)]\tLoss: 0.246987\n","Train Epoch: 5 [10400/33600 (31%)]\tLoss: 0.026243\n","Train Epoch: 5 [11200/33600 (33%)]\tLoss: 0.044153\n","Train Epoch: 5 [12000/33600 (36%)]\tLoss: 0.056188\n","Train Epoch: 5 [12800/33600 (38%)]\tLoss: 0.253622\n","Train Epoch: 5 [13600/33600 (40%)]\tLoss: 0.423329\n","Train Epoch: 5 [14400/33600 (43%)]\tLoss: 0.353745\n","Train Epoch: 5 [15200/33600 (45%)]\tLoss: 0.088886\n","Train Epoch: 5 [16000/33600 (48%)]\tLoss: 0.014646\n","Train Epoch: 5 [16800/33600 (50%)]\tLoss: 1.135426\n","Train Epoch: 5 [17600/33600 (52%)]\tLoss: 0.007387\n","Train Epoch: 5 [18400/33600 (55%)]\tLoss: 0.049682\n","Train Epoch: 5 [19200/33600 (57%)]\tLoss: 0.772433\n","Train Epoch: 5 [20000/33600 (60%)]\tLoss: 0.066210\n","Train Epoch: 5 [20800/33600 (62%)]\tLoss: 0.669262\n","Train Epoch: 5 [21600/33600 (64%)]\tLoss: 0.978997\n","Train Epoch: 5 [22400/33600 (67%)]\tLoss: 0.068135\n","Train Epoch: 5 [23200/33600 (69%)]\tLoss: 0.007525\n","Train Epoch: 5 [24000/33600 (71%)]\tLoss: 1.033707\n","Train Epoch: 5 [24800/33600 (74%)]\tLoss: 0.687180\n","Train Epoch: 5 [25600/33600 (76%)]\tLoss: 0.098185\n","Train Epoch: 5 [26400/33600 (79%)]\tLoss: 0.222485\n","Train Epoch: 5 [27200/33600 (81%)]\tLoss: 1.302727\n","Train Epoch: 5 [28000/33600 (83%)]\tLoss: 0.085586\n","Train Epoch: 5 [28800/33600 (86%)]\tLoss: 0.021750\n","Train Epoch: 5 [29600/33600 (88%)]\tLoss: 0.072707\n","Train Epoch: 5 [30400/33600 (90%)]\tLoss: 0.126589\n","Train Epoch: 5 [31200/33600 (93%)]\tLoss: 0.075835\n","Train Epoch: 5 [32000/33600 (95%)]\tLoss: 0.100919\n","Train Epoch: 5 [32800/33600 (98%)]\tLoss: 0.992587\n","Train Epoch: 5 [33600/33600 (100%)]\tLoss: 0.310397\n","\n","Average Val Loss: 0.0348, Val Accuracy: 8312/8400 (98.952%)\n","\n","Train Epoch: 6 [800/33600 (2%)]\tLoss: 0.203295\n","Train Epoch: 6 [1600/33600 (5%)]\tLoss: 0.014259\n","Train Epoch: 6 [2400/33600 (7%)]\tLoss: 0.121602\n","Train Epoch: 6 [3200/33600 (10%)]\tLoss: 1.087547\n","Train Epoch: 6 [4000/33600 (12%)]\tLoss: 0.055370\n","Train Epoch: 6 [4800/33600 (14%)]\tLoss: 1.118233\n","Train Epoch: 6 [5600/33600 (17%)]\tLoss: 0.051316\n","Train Epoch: 6 [6400/33600 (19%)]\tLoss: 0.304490\n","Train Epoch: 6 [7200/33600 (21%)]\tLoss: 0.008752\n","Train Epoch: 6 [8000/33600 (24%)]\tLoss: 0.026139\n","Train Epoch: 6 [8800/33600 (26%)]\tLoss: 0.055484\n","Train Epoch: 6 [9600/33600 (29%)]\tLoss: 0.456732\n","Train Epoch: 6 [10400/33600 (31%)]\tLoss: 0.170283\n","Train Epoch: 6 [11200/33600 (33%)]\tLoss: 0.025678\n","Train Epoch: 6 [12000/33600 (36%)]\tLoss: 0.734332\n","Train Epoch: 6 [12800/33600 (38%)]\tLoss: 0.076654\n","Train Epoch: 6 [13600/33600 (40%)]\tLoss: 0.013358\n","Train Epoch: 6 [14400/33600 (43%)]\tLoss: 0.113596\n","Train Epoch: 6 [15200/33600 (45%)]\tLoss: 0.030772\n","Train Epoch: 6 [16000/33600 (48%)]\tLoss: 0.026725\n","Train Epoch: 6 [16800/33600 (50%)]\tLoss: 0.058225\n","Train Epoch: 6 [17600/33600 (52%)]\tLoss: 0.084174\n","Train Epoch: 6 [18400/33600 (55%)]\tLoss: 0.110105\n","Train Epoch: 6 [19200/33600 (57%)]\tLoss: 0.059842\n","Train Epoch: 6 [20000/33600 (60%)]\tLoss: 0.289208\n","Train Epoch: 6 [20800/33600 (62%)]\tLoss: 0.005631\n","Train Epoch: 6 [21600/33600 (64%)]\tLoss: 1.118732\n","Train Epoch: 6 [22400/33600 (67%)]\tLoss: 0.086635\n","Train Epoch: 6 [23200/33600 (69%)]\tLoss: 0.418669\n","Train Epoch: 6 [24000/33600 (71%)]\tLoss: 0.027587\n","Train Epoch: 6 [24800/33600 (74%)]\tLoss: 0.248932\n","Train Epoch: 6 [25600/33600 (76%)]\tLoss: 0.058038\n","Train Epoch: 6 [26400/33600 (79%)]\tLoss: 0.068154\n","Train Epoch: 6 [27200/33600 (81%)]\tLoss: 0.329766\n","Train Epoch: 6 [28000/33600 (83%)]\tLoss: 0.150599\n","Train Epoch: 6 [28800/33600 (86%)]\tLoss: 0.200052\n","Train Epoch: 6 [29600/33600 (88%)]\tLoss: 0.066349\n","Train Epoch: 6 [30400/33600 (90%)]\tLoss: 0.141462\n","Train Epoch: 6 [31200/33600 (93%)]\tLoss: 0.249864\n","Train Epoch: 6 [32000/33600 (95%)]\tLoss: 0.099785\n","Train Epoch: 6 [32800/33600 (98%)]\tLoss: 0.087775\n","Train Epoch: 6 [33600/33600 (100%)]\tLoss: 0.049183\n","\n","Average Val Loss: 0.0312, Val Accuracy: 8328/8400 (99.143%)\n","\n","Train Epoch: 7 [800/33600 (2%)]\tLoss: 0.013931\n","Train Epoch: 7 [1600/33600 (5%)]\tLoss: 0.376297\n","Train Epoch: 7 [2400/33600 (7%)]\tLoss: 0.084511\n","Train Epoch: 7 [3200/33600 (10%)]\tLoss: 0.183213\n","Train Epoch: 7 [4000/33600 (12%)]\tLoss: 0.019027\n","Train Epoch: 7 [4800/33600 (14%)]\tLoss: 0.138020\n","Train Epoch: 7 [5600/33600 (17%)]\tLoss: 0.226563\n","Train Epoch: 7 [6400/33600 (19%)]\tLoss: 0.005854\n","Train Epoch: 7 [7200/33600 (21%)]\tLoss: 0.023417\n","Train Epoch: 7 [8000/33600 (24%)]\tLoss: 0.150595\n","Train Epoch: 7 [8800/33600 (26%)]\tLoss: 0.017191\n","Train Epoch: 7 [9600/33600 (29%)]\tLoss: 0.027819\n","Train Epoch: 7 [10400/33600 (31%)]\tLoss: 0.150346\n","Train Epoch: 7 [11200/33600 (33%)]\tLoss: 0.215230\n","Train Epoch: 7 [12000/33600 (36%)]\tLoss: 0.197124\n","Train Epoch: 7 [12800/33600 (38%)]\tLoss: 0.108053\n","Train Epoch: 7 [13600/33600 (40%)]\tLoss: 0.027789\n","Train Epoch: 7 [14400/33600 (43%)]\tLoss: 0.074420\n","Train Epoch: 7 [15200/33600 (45%)]\tLoss: 0.025951\n","Train Epoch: 7 [16000/33600 (48%)]\tLoss: 0.348138\n","Train Epoch: 7 [16800/33600 (50%)]\tLoss: 0.009041\n","Train Epoch: 7 [17600/33600 (52%)]\tLoss: 0.009440\n","Train Epoch: 7 [18400/33600 (55%)]\tLoss: 0.019865\n","Train Epoch: 7 [19200/33600 (57%)]\tLoss: 0.021554\n","Train Epoch: 7 [20000/33600 (60%)]\tLoss: 0.288545\n","Train Epoch: 7 [20800/33600 (62%)]\tLoss: 0.127982\n","Train Epoch: 7 [21600/33600 (64%)]\tLoss: 0.002176\n","Train Epoch: 7 [22400/33600 (67%)]\tLoss: 0.008509\n","Train Epoch: 7 [23200/33600 (69%)]\tLoss: 0.304187\n","Train Epoch: 7 [24000/33600 (71%)]\tLoss: 0.053087\n","Train Epoch: 7 [24800/33600 (74%)]\tLoss: 0.066232\n","Train Epoch: 7 [25600/33600 (76%)]\tLoss: 0.010684\n","Train Epoch: 7 [26400/33600 (79%)]\tLoss: 0.320411\n","Train Epoch: 7 [27200/33600 (81%)]\tLoss: 0.024068\n","Train Epoch: 7 [28000/33600 (83%)]\tLoss: 0.043505\n","Train Epoch: 7 [28800/33600 (86%)]\tLoss: 0.030916\n","Train Epoch: 7 [29600/33600 (88%)]\tLoss: 0.137195\n","Train Epoch: 7 [30400/33600 (90%)]\tLoss: 0.264514\n","Train Epoch: 7 [31200/33600 (93%)]\tLoss: 0.349049\n","Train Epoch: 7 [32000/33600 (95%)]\tLoss: 0.031234\n","Train Epoch: 7 [32800/33600 (98%)]\tLoss: 0.007210\n","Train Epoch: 7 [33600/33600 (100%)]\tLoss: 0.201870\n","\n","Average Val Loss: 0.0305, Val Accuracy: 8328/8400 (99.143%)\n","\n","Train Epoch: 8 [800/33600 (2%)]\tLoss: 0.011758\n","Train Epoch: 8 [1600/33600 (5%)]\tLoss: 0.088949\n","Train Epoch: 8 [2400/33600 (7%)]\tLoss: 0.622318\n","Train Epoch: 8 [3200/33600 (10%)]\tLoss: 0.447816\n","Train Epoch: 8 [4000/33600 (12%)]\tLoss: 0.049209\n","Train Epoch: 8 [4800/33600 (14%)]\tLoss: 0.056397\n","Train Epoch: 8 [5600/33600 (17%)]\tLoss: 0.090992\n","Train Epoch: 8 [6400/33600 (19%)]\tLoss: 0.021375\n","Train Epoch: 8 [7200/33600 (21%)]\tLoss: 0.004249\n","Train Epoch: 8 [8000/33600 (24%)]\tLoss: 0.022524\n","Train Epoch: 8 [8800/33600 (26%)]\tLoss: 0.010964\n","Train Epoch: 8 [9600/33600 (29%)]\tLoss: 0.108989\n","Train Epoch: 8 [10400/33600 (31%)]\tLoss: 0.009555\n","Train Epoch: 8 [11200/33600 (33%)]\tLoss: 0.047643\n","Train Epoch: 8 [12000/33600 (36%)]\tLoss: 0.190428\n","Train Epoch: 8 [12800/33600 (38%)]\tLoss: 0.169589\n","Train Epoch: 8 [13600/33600 (40%)]\tLoss: 0.581665\n","Train Epoch: 8 [14400/33600 (43%)]\tLoss: 0.012970\n","Train Epoch: 8 [15200/33600 (45%)]\tLoss: 0.027814\n","Train Epoch: 8 [16000/33600 (48%)]\tLoss: 0.023365\n","Train Epoch: 8 [16800/33600 (50%)]\tLoss: 0.016241\n","Train Epoch: 8 [17600/33600 (52%)]\tLoss: 0.063115\n","Train Epoch: 8 [18400/33600 (55%)]\tLoss: 0.402732\n","Train Epoch: 8 [19200/33600 (57%)]\tLoss: 0.085087\n","Train Epoch: 8 [20000/33600 (60%)]\tLoss: 0.183553\n","Train Epoch: 8 [20800/33600 (62%)]\tLoss: 0.015436\n","Train Epoch: 8 [21600/33600 (64%)]\tLoss: 0.891580\n","Train Epoch: 8 [22400/33600 (67%)]\tLoss: 0.058364\n","Train Epoch: 8 [23200/33600 (69%)]\tLoss: 0.043734\n","Train Epoch: 8 [24000/33600 (71%)]\tLoss: 0.009608\n","Train Epoch: 8 [24800/33600 (74%)]\tLoss: 0.435760\n","Train Epoch: 8 [25600/33600 (76%)]\tLoss: 0.023583\n","Train Epoch: 8 [26400/33600 (79%)]\tLoss: 0.030084\n","Train Epoch: 8 [27200/33600 (81%)]\tLoss: 0.049552\n","Train Epoch: 8 [28000/33600 (83%)]\tLoss: 0.010935\n","Train Epoch: 8 [28800/33600 (86%)]\tLoss: 0.029730\n","Train Epoch: 8 [29600/33600 (88%)]\tLoss: 0.005382\n","Train Epoch: 8 [30400/33600 (90%)]\tLoss: 0.105725\n","Train Epoch: 8 [31200/33600 (93%)]\tLoss: 0.024908\n","Train Epoch: 8 [32000/33600 (95%)]\tLoss: 0.004786\n","Train Epoch: 8 [32800/33600 (98%)]\tLoss: 0.019236\n","Train Epoch: 8 [33600/33600 (100%)]\tLoss: 0.036968\n","\n","Average Val Loss: 0.0292, Val Accuracy: 8332/8400 (99.190%)\n","\n","Train Epoch: 9 [800/33600 (2%)]\tLoss: 0.030831\n","Train Epoch: 9 [1600/33600 (5%)]\tLoss: 0.133187\n","Train Epoch: 9 [2400/33600 (7%)]\tLoss: 0.021638\n","Train Epoch: 9 [3200/33600 (10%)]\tLoss: 0.038599\n","Train Epoch: 9 [4000/33600 (12%)]\tLoss: 0.157680\n","Train Epoch: 9 [4800/33600 (14%)]\tLoss: 0.067637\n","Train Epoch: 9 [5600/33600 (17%)]\tLoss: 0.122027\n","Train Epoch: 9 [6400/33600 (19%)]\tLoss: 0.011518\n","Train Epoch: 9 [7200/33600 (21%)]\tLoss: 0.939733\n","Train Epoch: 9 [8000/33600 (24%)]\tLoss: 0.325872\n","Train Epoch: 9 [8800/33600 (26%)]\tLoss: 0.006491\n","Train Epoch: 9 [9600/33600 (29%)]\tLoss: 0.063644\n","Train Epoch: 9 [10400/33600 (31%)]\tLoss: 0.109752\n","Train Epoch: 9 [11200/33600 (33%)]\tLoss: 0.010794\n","Train Epoch: 9 [12000/33600 (36%)]\tLoss: 0.143749\n","Train Epoch: 9 [12800/33600 (38%)]\tLoss: 0.278120\n","Train Epoch: 9 [13600/33600 (40%)]\tLoss: 0.004852\n","Train Epoch: 9 [14400/33600 (43%)]\tLoss: 0.101072\n","Train Epoch: 9 [15200/33600 (45%)]\tLoss: 0.055035\n","Train Epoch: 9 [16000/33600 (48%)]\tLoss: 0.007653\n","Train Epoch: 9 [16800/33600 (50%)]\tLoss: 0.033326\n","Train Epoch: 9 [17600/33600 (52%)]\tLoss: 0.468086\n","Train Epoch: 9 [18400/33600 (55%)]\tLoss: 0.008356\n","Train Epoch: 9 [19200/33600 (57%)]\tLoss: 0.584724\n","Train Epoch: 9 [20000/33600 (60%)]\tLoss: 0.559535\n","Train Epoch: 9 [20800/33600 (62%)]\tLoss: 0.128316\n","Train Epoch: 9 [21600/33600 (64%)]\tLoss: 0.184103\n","Train Epoch: 9 [22400/33600 (67%)]\tLoss: 0.009527\n","Train Epoch: 9 [23200/33600 (69%)]\tLoss: 0.159966\n","Train Epoch: 9 [24000/33600 (71%)]\tLoss: 0.062035\n","Train Epoch: 9 [24800/33600 (74%)]\tLoss: 0.001570\n","Train Epoch: 9 [25600/33600 (76%)]\tLoss: 0.083073\n","Train Epoch: 9 [26400/33600 (79%)]\tLoss: 0.324885\n","Train Epoch: 9 [27200/33600 (81%)]\tLoss: 0.009202\n","Train Epoch: 9 [28000/33600 (83%)]\tLoss: 0.012067\n","Train Epoch: 9 [28800/33600 (86%)]\tLoss: 0.012240\n","Train Epoch: 9 [29600/33600 (88%)]\tLoss: 1.003775\n","Train Epoch: 9 [30400/33600 (90%)]\tLoss: 0.322352\n","Train Epoch: 9 [31200/33600 (93%)]\tLoss: 0.029240\n","Train Epoch: 9 [32000/33600 (95%)]\tLoss: 0.011838\n","Train Epoch: 9 [32800/33600 (98%)]\tLoss: 0.173215\n","Train Epoch: 9 [33600/33600 (100%)]\tLoss: 0.000847\n","\n","Average Val Loss: 0.0275, Val Accuracy: 8338/8400 (99.262%)\n","\n"]}],"source":["num_epochs = 10\n","\n","for n in range(num_epochs):\n","    train_model(n)\n","    evaluate(val_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Saving the full CNN model"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/miniconda3/envs/ef_env/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"]}],"source":["# Save the full model\n","torch.save(conv_model, 'cnn_full_model.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the full CNN model"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"data":{"text/plain":["Net(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout1): Dropout(p=0.5)\n","  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n","  (bn_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout2): Dropout(p=0.5)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (bn_fc2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout3): Dropout(p=0.5)\n","  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",")"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["# To load the full model after the JN has timed out:\n","conv_model = torch.load('cnn_full_model.pth')\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","conv_model.eval()  # Setting the model to evaluation mode"]},{"cell_type":"markdown","metadata":{"_uuid":"ba3507e41b548d5a7f6005a6eeb9219f246dea7c"},"source":["#### Make predictions on the test set (just the CNN)"]},{"cell_type":"code","execution_count":95,"metadata":{"_uuid":"4fedc4c69ff64bf3108a291926643fffce1c864e","trusted":true},"outputs":[],"source":["def make_predictions(data_loader):\n","    conv_model.eval()\n","    test_preds = torch.LongTensor()\n","    \n","    for i, data in enumerate(data_loader):\n","        data = data.unsqueeze(1)\n","        \n","        if torch.cuda.is_available():\n","            data = data.cuda()\n","            \n","        output = conv_model(data)\n","        \n","        preds = output.cpu().data.max(1, keepdim=True)[1]\n","        test_preds = torch.cat((test_preds, preds), dim=0)\n","        \n","    return test_preds\n","\n","#Run the inference\n","test_set_preds = make_predictions(test_loader)"]},{"cell_type":"markdown","metadata":{"_uuid":"80fae08d574b70975df07189687853b606389d74"},"source":["#### Prepare Submissions"]},{"cell_type":"code","execution_count":58,"metadata":{"_uuid":"d9aa65de4472cdc45de649cc55b95bf4e9435cdb","trusted":true},"outputs":[],"source":["submission_df = pd.read_csv(\"../Data/sample_submission.csv\")"]},{"cell_type":"code","execution_count":59,"metadata":{"_uuid":"6e6ca4d2373511e3d6bed6054a54464267ee0f14","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageId</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ImageId  Label\n","0        1      2\n","1        2      0\n","2        3      9\n","3        4      0\n","4        5      3"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["submission_df['Label'] = test_set_preds.numpy().squeeze()\n","submission_df.head()"]},{"cell_type":"code","execution_count":60,"metadata":{"_uuid":"bb7b97c22c0e1a94796e3868834bf8ac0f213562","trusted":true},"outputs":[],"source":["submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Puppeteer MLP"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["class PuppeteerNet(nn.Module):\n","    def __init__(self):\n","        super(PuppeteerNet, self).__init__()\n","        \n","        # Shared Base MLP\n","        self.base_mlp = nn.Sequential(\n","            nn.Linear(10, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","        )\n","        \n","        # Layer-Specific Adjustment Heads\n","        # Note: Each adjustment head must handle batch processing\n","        # conv1 adjustment\n","        self.conv1_adjustment = nn.Linear(512, 32 * 28 * 28)\n","        # conv2 adjustment\n","        self.conv2_adjustment = nn.Linear(512, 64 * 28 * 28)\n","        # conv3 adjustment\n","        self.conv3_adjustment = nn.Linear(512, 128 * 14 * 14)\n","        # fc1 adjustment\n","        self.fc1_adjustment = nn.Linear(512, 128)\n","        # fc2 adjustment\n","        self.fc2_adjustment = nn.Linear(512, 64)\n","        \n","        # Activation Function\n","        self.activation = nn.Tanh()\n","        \n","    def forward(self, x):\n","        base_out = self.base_mlp(x)\n","        adjustments = {}\n","        \n","        # conv1 adjustment\n","        adj_conv1 = self.conv1_adjustment(base_out)\n","        adj_conv1 = self.activation(adj_conv1)\n","        adjustments['conv1'] = adj_conv1.view(-1, 32, 28, 28)\n","        \n","        # conv2 adjustment\n","        adj_conv2 = self.conv2_adjustment(base_out)\n","        adj_conv2 = self.activation(adj_conv2)\n","        adjustments['conv2'] = adj_conv2.view(-1, 64, 28, 28)\n","        \n","        # conv3 adjustment\n","        adj_conv3 = self.conv3_adjustment(base_out)\n","        adj_conv3 = self.activation(adj_conv3)\n","        adjustments['conv3'] = adj_conv3.view(-1, 128, 14, 14)\n","        \n","        # fc1 adjustment\n","        adj_fc1 = self.fc1_adjustment(base_out)\n","        adj_fc1 = self.activation(adj_fc1)\n","        adjustments['fc1'] = adj_fc1  # Shape: [batch_size, 128]\n","        \n","        # fc2 adjustment\n","        adj_fc2 = self.fc2_adjustment(base_out)\n","        adj_fc2 = self.activation(adj_fc2)\n","        adjustments['fc2'] = adj_fc2  # Shape: [batch_size, 64]\n","        \n","        return adjustments\n"]},{"cell_type":"markdown","metadata":{},"source":["### Puppeteer model Inference"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Step [100/4200], Total Loss: 1.7873, Classification Loss: 1.7591, Inhibition Loss: 0.0282, Accuracy: 74.75%, Avg p_inhibited: 0.0100\n","Epoch [1/10], Step [200/4200], Total Loss: 1.5494, Classification Loss: 1.5210, Inhibition Loss: 0.0284, Accuracy: 74.25%, Avg p_inhibited: 0.0089\n","Epoch [1/10], Step [300/4200], Total Loss: 0.9446, Classification Loss: 0.9421, Inhibition Loss: 0.0026, Accuracy: 82.12%, Avg p_inhibited: 0.0023\n","Epoch [1/10], Step [400/4200], Total Loss: 0.5992, Classification Loss: 0.5954, Inhibition Loss: 0.0038, Accuracy: 87.62%, Avg p_inhibited: 0.0015\n","Epoch [1/10], Step [500/4200], Total Loss: 0.4391, Classification Loss: 0.4340, Inhibition Loss: 0.0051, Accuracy: 89.38%, Avg p_inhibited: 0.0027\n","Epoch [1/10], Step [600/4200], Total Loss: 0.4673, Classification Loss: 0.4636, Inhibition Loss: 0.0038, Accuracy: 88.88%, Avg p_inhibited: 0.0031\n","Epoch [1/10], Step [700/4200], Total Loss: 0.4492, Classification Loss: 0.4476, Inhibition Loss: 0.0017, Accuracy: 89.25%, Avg p_inhibited: 0.0012\n","Epoch [1/10], Step [800/4200], Total Loss: 0.4154, Classification Loss: 0.4107, Inhibition Loss: 0.0047, Accuracy: 90.50%, Avg p_inhibited: 0.0019\n","Epoch [1/10], Step [900/4200], Total Loss: 0.2508, Classification Loss: 0.2493, Inhibition Loss: 0.0015, Accuracy: 93.50%, Avg p_inhibited: 0.0012\n","Epoch [1/10], Step [1000/4200], Total Loss: 0.3502, Classification Loss: 0.3497, Inhibition Loss: 0.0005, Accuracy: 91.88%, Avg p_inhibited: 0.0005\n","Epoch [1/10], Step [1100/4200], Total Loss: 0.2241, Classification Loss: 0.2198, Inhibition Loss: 0.0043, Accuracy: 94.88%, Avg p_inhibited: 0.0030\n","Epoch [1/10], Step [1200/4200], Total Loss: 0.2161, Classification Loss: 0.2148, Inhibition Loss: 0.0012, Accuracy: 93.62%, Avg p_inhibited: 0.0009\n","Epoch [1/10], Step [1300/4200], Total Loss: 0.2518, Classification Loss: 0.2505, Inhibition Loss: 0.0013, Accuracy: 93.12%, Avg p_inhibited: 0.0012\n","Epoch [1/10], Step [1400/4200], Total Loss: 0.2806, Classification Loss: 0.2761, Inhibition Loss: 0.0045, Accuracy: 92.62%, Avg p_inhibited: 0.0025\n","Epoch [1/10], Step [1500/4200], Total Loss: 0.2344, Classification Loss: 0.2330, Inhibition Loss: 0.0014, Accuracy: 93.25%, Avg p_inhibited: 0.0011\n","Epoch [1/10], Step [1600/4200], Total Loss: 0.2071, Classification Loss: 0.2052, Inhibition Loss: 0.0019, Accuracy: 94.62%, Avg p_inhibited: 0.0015\n","Epoch [1/10], Step [1700/4200], Total Loss: 0.2259, Classification Loss: 0.2225, Inhibition Loss: 0.0034, Accuracy: 93.25%, Avg p_inhibited: 0.0021\n","Epoch [1/10], Step [1800/4200], Total Loss: 0.2326, Classification Loss: 0.2293, Inhibition Loss: 0.0034, Accuracy: 93.88%, Avg p_inhibited: 0.0023\n","Epoch [1/10], Step [1900/4200], Total Loss: 0.1277, Classification Loss: 0.1273, Inhibition Loss: 0.0004, Accuracy: 96.00%, Avg p_inhibited: 0.0004\n","Epoch [1/10], Step [2000/4200], Total Loss: 0.1574, Classification Loss: 0.1543, Inhibition Loss: 0.0031, Accuracy: 95.75%, Avg p_inhibited: 0.0019\n","Epoch [1/10], Step [2100/4200], Total Loss: 0.1322, Classification Loss: 0.1321, Inhibition Loss: 0.0001, Accuracy: 95.88%, Avg p_inhibited: 0.0001\n","Epoch [1/10], Step [2200/4200], Total Loss: 0.1474, Classification Loss: 0.1457, Inhibition Loss: 0.0017, Accuracy: 95.62%, Avg p_inhibited: 0.0013\n","Epoch [1/10], Step [2300/4200], Total Loss: 0.1200, Classification Loss: 0.1174, Inhibition Loss: 0.0027, Accuracy: 96.38%, Avg p_inhibited: 0.0014\n","Epoch [1/10], Step [2400/4200], Total Loss: 0.1189, Classification Loss: 0.1178, Inhibition Loss: 0.0010, Accuracy: 95.75%, Avg p_inhibited: 0.0009\n","Epoch [1/10], Step [2500/4200], Total Loss: 0.0985, Classification Loss: 0.0982, Inhibition Loss: 0.0004, Accuracy: 96.50%, Avg p_inhibited: 0.0004\n","Epoch [1/10], Step [2600/4200], Total Loss: 0.1589, Classification Loss: 0.1551, Inhibition Loss: 0.0038, Accuracy: 95.75%, Avg p_inhibited: 0.0021\n","Epoch [1/10], Step [2700/4200], Total Loss: 0.1255, Classification Loss: 0.1252, Inhibition Loss: 0.0003, Accuracy: 96.50%, Avg p_inhibited: 0.0003\n","Epoch [1/10], Step [2800/4200], Total Loss: 0.1187, Classification Loss: 0.1159, Inhibition Loss: 0.0028, Accuracy: 96.25%, Avg p_inhibited: 0.0014\n","Epoch [1/10], Step [2900/4200], Total Loss: 0.1112, Classification Loss: 0.1064, Inhibition Loss: 0.0048, Accuracy: 97.12%, Avg p_inhibited: 0.0014\n","Epoch [1/10], Step [3000/4200], Total Loss: 0.0766, Classification Loss: 0.0761, Inhibition Loss: 0.0005, Accuracy: 97.38%, Avg p_inhibited: 0.0005\n","Epoch [1/10], Step [3100/4200], Total Loss: 0.0319, Classification Loss: 0.0317, Inhibition Loss: 0.0001, Accuracy: 98.88%, Avg p_inhibited: 0.0001\n","Epoch [1/10], Step [3200/4200], Total Loss: 0.0796, Classification Loss: 0.0765, Inhibition Loss: 0.0031, Accuracy: 97.12%, Avg p_inhibited: 0.0020\n","Epoch [1/10], Step [3300/4200], Total Loss: 0.0987, Classification Loss: 0.0982, Inhibition Loss: 0.0005, Accuracy: 96.62%, Avg p_inhibited: 0.0005\n","Epoch [1/10], Step [3400/4200], Total Loss: 0.0697, Classification Loss: 0.0684, Inhibition Loss: 0.0013, Accuracy: 98.25%, Avg p_inhibited: 0.0011\n","Epoch [1/10], Step [3500/4200], Total Loss: 0.0995, Classification Loss: 0.0984, Inhibition Loss: 0.0010, Accuracy: 97.00%, Avg p_inhibited: 0.0010\n","Epoch [1/10], Step [3600/4200], Total Loss: 0.0692, Classification Loss: 0.0688, Inhibition Loss: 0.0003, Accuracy: 97.75%, Avg p_inhibited: 0.0003\n","Epoch [1/10], Step [3700/4200], Total Loss: 0.1182, Classification Loss: 0.1178, Inhibition Loss: 0.0004, Accuracy: 96.25%, Avg p_inhibited: 0.0004\n","Epoch [1/10], Step [3800/4200], Total Loss: 0.0688, Classification Loss: 0.0647, Inhibition Loss: 0.0041, Accuracy: 97.88%, Avg p_inhibited: 0.0020\n","Epoch [1/10], Step [3900/4200], Total Loss: 0.0825, Classification Loss: 0.0821, Inhibition Loss: 0.0005, Accuracy: 97.75%, Avg p_inhibited: 0.0005\n","Epoch [1/10], Step [4000/4200], Total Loss: 0.0682, Classification Loss: 0.0649, Inhibition Loss: 0.0034, Accuracy: 98.25%, Avg p_inhibited: 0.0014\n","Epoch [1/10], Step [4100/4200], Total Loss: 0.0596, Classification Loss: 0.0587, Inhibition Loss: 0.0008, Accuracy: 98.62%, Avg p_inhibited: 0.0008\n","Epoch [1/10], Step [4200/4200], Total Loss: 0.1040, Classification Loss: 0.1015, Inhibition Loss: 0.0024, Accuracy: 97.12%, Avg p_inhibited: 0.0016\n","Epoch [2/10], Step [100/4200], Total Loss: 0.0408, Classification Loss: 0.0405, Inhibition Loss: 0.0003, Accuracy: 98.75%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [200/4200], Total Loss: 0.0320, Classification Loss: 0.0316, Inhibition Loss: 0.0004, Accuracy: 98.88%, Avg p_inhibited: 0.0004\n","Epoch [2/10], Step [300/4200], Total Loss: 0.0567, Classification Loss: 0.0563, Inhibition Loss: 0.0004, Accuracy: 98.12%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [400/4200], Total Loss: 0.0840, Classification Loss: 0.0818, Inhibition Loss: 0.0022, Accuracy: 97.00%, Avg p_inhibited: 0.0015\n","Epoch [2/10], Step [500/4200], Total Loss: 0.0363, Classification Loss: 0.0359, Inhibition Loss: 0.0004, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [2/10], Step [600/4200], Total Loss: 0.0508, Classification Loss: 0.0502, Inhibition Loss: 0.0006, Accuracy: 98.38%, Avg p_inhibited: 0.0006\n","Epoch [2/10], Step [700/4200], Total Loss: 0.0377, Classification Loss: 0.0374, Inhibition Loss: 0.0003, Accuracy: 98.62%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [800/4200], Total Loss: 0.0467, Classification Loss: 0.0449, Inhibition Loss: 0.0018, Accuracy: 98.12%, Avg p_inhibited: 0.0011\n","Epoch [2/10], Step [900/4200], Total Loss: 0.0575, Classification Loss: 0.0526, Inhibition Loss: 0.0049, Accuracy: 98.38%, Avg p_inhibited: 0.0027\n","Epoch [2/10], Step [1000/4200], Total Loss: 0.0382, Classification Loss: 0.0380, Inhibition Loss: 0.0002, Accuracy: 98.75%, Avg p_inhibited: 0.0002\n","Epoch [2/10], Step [1100/4200], Total Loss: 0.0451, Classification Loss: 0.0437, Inhibition Loss: 0.0014, Accuracy: 98.88%, Avg p_inhibited: 0.0012\n","Epoch [2/10], Step [1200/4200], Total Loss: 0.0404, Classification Loss: 0.0397, Inhibition Loss: 0.0007, Accuracy: 98.75%, Avg p_inhibited: 0.0006\n","Epoch [2/10], Step [1300/4200], Total Loss: 0.0299, Classification Loss: 0.0298, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [2/10], Step [1400/4200], Total Loss: 0.0807, Classification Loss: 0.0805, Inhibition Loss: 0.0002, Accuracy: 97.88%, Avg p_inhibited: 0.0002\n","Epoch [2/10], Step [1500/4200], Total Loss: 0.0389, Classification Loss: 0.0386, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [1600/4200], Total Loss: 0.0564, Classification Loss: 0.0561, Inhibition Loss: 0.0003, Accuracy: 98.25%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [1700/4200], Total Loss: 0.0552, Classification Loss: 0.0509, Inhibition Loss: 0.0042, Accuracy: 98.38%, Avg p_inhibited: 0.0014\n","Epoch [2/10], Step [1800/4200], Total Loss: 0.0465, Classification Loss: 0.0454, Inhibition Loss: 0.0011, Accuracy: 98.50%, Avg p_inhibited: 0.0008\n","Epoch [2/10], Step [1900/4200], Total Loss: 0.0223, Classification Loss: 0.0221, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [2/10], Step [2000/4200], Total Loss: 0.0780, Classification Loss: 0.0777, Inhibition Loss: 0.0004, Accuracy: 98.00%, Avg p_inhibited: 0.0004\n","Epoch [2/10], Step [2100/4200], Total Loss: 0.0437, Classification Loss: 0.0425, Inhibition Loss: 0.0012, Accuracy: 98.50%, Avg p_inhibited: 0.0009\n","Epoch [2/10], Step [2200/4200], Total Loss: 0.0646, Classification Loss: 0.0636, Inhibition Loss: 0.0010, Accuracy: 98.25%, Avg p_inhibited: 0.0009\n","Epoch [2/10], Step [2300/4200], Total Loss: 0.0661, Classification Loss: 0.0657, Inhibition Loss: 0.0003, Accuracy: 98.00%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [2400/4200], Total Loss: 0.0441, Classification Loss: 0.0437, Inhibition Loss: 0.0004, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [2/10], Step [2500/4200], Total Loss: 0.0302, Classification Loss: 0.0299, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [2600/4200], Total Loss: 0.0255, Classification Loss: 0.0253, Inhibition Loss: 0.0002, Accuracy: 99.12%, Avg p_inhibited: 0.0002\n","Epoch [2/10], Step [2700/4200], Total Loss: 0.0454, Classification Loss: 0.0441, Inhibition Loss: 0.0013, Accuracy: 98.88%, Avg p_inhibited: 0.0011\n","Epoch [2/10], Step [2800/4200], Total Loss: 0.0255, Classification Loss: 0.0251, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [2900/4200], Total Loss: 0.0442, Classification Loss: 0.0437, Inhibition Loss: 0.0005, Accuracy: 98.50%, Avg p_inhibited: 0.0005\n","Epoch [2/10], Step [3000/4200], Total Loss: 0.0632, Classification Loss: 0.0624, Inhibition Loss: 0.0008, Accuracy: 98.12%, Avg p_inhibited: 0.0007\n","Epoch [2/10], Step [3100/4200], Total Loss: 0.0425, Classification Loss: 0.0421, Inhibition Loss: 0.0004, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [2/10], Step [3200/4200], Total Loss: 0.0442, Classification Loss: 0.0439, Inhibition Loss: 0.0003, Accuracy: 98.38%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [3300/4200], Total Loss: 0.0487, Classification Loss: 0.0467, Inhibition Loss: 0.0020, Accuracy: 98.38%, Avg p_inhibited: 0.0014\n","Epoch [2/10], Step [3400/4200], Total Loss: 0.0429, Classification Loss: 0.0410, Inhibition Loss: 0.0018, Accuracy: 98.50%, Avg p_inhibited: 0.0014\n","Epoch [2/10], Step [3500/4200], Total Loss: 0.0563, Classification Loss: 0.0557, Inhibition Loss: 0.0006, Accuracy: 98.12%, Avg p_inhibited: 0.0006\n","Epoch [2/10], Step [3600/4200], Total Loss: 0.0253, Classification Loss: 0.0238, Inhibition Loss: 0.0015, Accuracy: 99.25%, Avg p_inhibited: 0.0013\n","Epoch [2/10], Step [3700/4200], Total Loss: 0.0362, Classification Loss: 0.0330, Inhibition Loss: 0.0031, Accuracy: 99.25%, Avg p_inhibited: 0.0017\n","Epoch [2/10], Step [3800/4200], Total Loss: 0.0536, Classification Loss: 0.0531, Inhibition Loss: 0.0006, Accuracy: 98.25%, Avg p_inhibited: 0.0005\n","Epoch [2/10], Step [3900/4200], Total Loss: 0.0286, Classification Loss: 0.0284, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [2/10], Step [4000/4200], Total Loss: 0.0356, Classification Loss: 0.0353, Inhibition Loss: 0.0003, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [2/10], Step [4100/4200], Total Loss: 0.0724, Classification Loss: 0.0722, Inhibition Loss: 0.0002, Accuracy: 98.38%, Avg p_inhibited: 0.0002\n","Epoch [2/10], Step [4200/4200], Total Loss: 0.0458, Classification Loss: 0.0450, Inhibition Loss: 0.0007, Accuracy: 98.38%, Avg p_inhibited: 0.0007\n","Epoch [3/10], Step [100/4200], Total Loss: 0.0220, Classification Loss: 0.0207, Inhibition Loss: 0.0012, Accuracy: 99.88%, Avg p_inhibited: 0.0010\n","Epoch [3/10], Step [200/4200], Total Loss: 0.0367, Classification Loss: 0.0363, Inhibition Loss: 0.0004, Accuracy: 98.25%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [300/4200], Total Loss: 0.0383, Classification Loss: 0.0371, Inhibition Loss: 0.0012, Accuracy: 98.88%, Avg p_inhibited: 0.0010\n","Epoch [3/10], Step [400/4200], Total Loss: 0.0523, Classification Loss: 0.0521, Inhibition Loss: 0.0002, Accuracy: 98.38%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [500/4200], Total Loss: 0.0259, Classification Loss: 0.0248, Inhibition Loss: 0.0011, Accuracy: 99.00%, Avg p_inhibited: 0.0008\n","Epoch [3/10], Step [600/4200], Total Loss: 0.0256, Classification Loss: 0.0253, Inhibition Loss: 0.0003, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [700/4200], Total Loss: 0.0403, Classification Loss: 0.0348, Inhibition Loss: 0.0055, Accuracy: 99.12%, Avg p_inhibited: 0.0018\n","Epoch [3/10], Step [800/4200], Total Loss: 0.0446, Classification Loss: 0.0436, Inhibition Loss: 0.0010, Accuracy: 99.00%, Avg p_inhibited: 0.0010\n","Epoch [3/10], Step [900/4200], Total Loss: 0.0591, Classification Loss: 0.0585, Inhibition Loss: 0.0006, Accuracy: 98.25%, Avg p_inhibited: 0.0005\n","Epoch [3/10], Step [1000/4200], Total Loss: 0.0270, Classification Loss: 0.0264, Inhibition Loss: 0.0006, Accuracy: 99.50%, Avg p_inhibited: 0.0005\n","Epoch [3/10], Step [1100/4200], Total Loss: 0.0369, Classification Loss: 0.0366, Inhibition Loss: 0.0003, Accuracy: 98.62%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [1200/4200], Total Loss: 0.0289, Classification Loss: 0.0287, Inhibition Loss: 0.0002, Accuracy: 98.62%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [1300/4200], Total Loss: 0.0271, Classification Loss: 0.0263, Inhibition Loss: 0.0008, Accuracy: 99.25%, Avg p_inhibited: 0.0007\n","Epoch [3/10], Step [1400/4200], Total Loss: 0.0283, Classification Loss: 0.0283, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [3/10], Step [1500/4200], Total Loss: 0.0439, Classification Loss: 0.0435, Inhibition Loss: 0.0004, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [1600/4200], Total Loss: 0.0356, Classification Loss: 0.0351, Inhibition Loss: 0.0006, Accuracy: 99.12%, Avg p_inhibited: 0.0005\n","Epoch [3/10], Step [1700/4200], Total Loss: 0.0335, Classification Loss: 0.0321, Inhibition Loss: 0.0014, Accuracy: 98.62%, Avg p_inhibited: 0.0011\n","Epoch [3/10], Step [1800/4200], Total Loss: 0.0332, Classification Loss: 0.0325, Inhibition Loss: 0.0007, Accuracy: 98.88%, Avg p_inhibited: 0.0007\n","Epoch [3/10], Step [1900/4200], Total Loss: 0.0263, Classification Loss: 0.0260, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [2000/4200], Total Loss: 0.0229, Classification Loss: 0.0226, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [2100/4200], Total Loss: 0.0308, Classification Loss: 0.0295, Inhibition Loss: 0.0013, Accuracy: 98.75%, Avg p_inhibited: 0.0009\n","Epoch [3/10], Step [2200/4200], Total Loss: 0.0313, Classification Loss: 0.0312, Inhibition Loss: 0.0002, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [2300/4200], Total Loss: 0.0405, Classification Loss: 0.0400, Inhibition Loss: 0.0006, Accuracy: 98.88%, Avg p_inhibited: 0.0005\n","Epoch [3/10], Step [2400/4200], Total Loss: 0.0277, Classification Loss: 0.0275, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [2500/4200], Total Loss: 0.0217, Classification Loss: 0.0203, Inhibition Loss: 0.0014, Accuracy: 99.50%, Avg p_inhibited: 0.0010\n","Epoch [3/10], Step [2600/4200], Total Loss: 0.0280, Classification Loss: 0.0276, Inhibition Loss: 0.0004, Accuracy: 98.62%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [2700/4200], Total Loss: 0.0329, Classification Loss: 0.0326, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [2800/4200], Total Loss: 0.0318, Classification Loss: 0.0314, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [2900/4200], Total Loss: 0.0341, Classification Loss: 0.0337, Inhibition Loss: 0.0004, Accuracy: 99.25%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [3000/4200], Total Loss: 0.0292, Classification Loss: 0.0291, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [3/10], Step [3100/4200], Total Loss: 0.0199, Classification Loss: 0.0195, Inhibition Loss: 0.0003, Accuracy: 99.38%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [3200/4200], Total Loss: 0.0393, Classification Loss: 0.0390, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [3300/4200], Total Loss: 0.0378, Classification Loss: 0.0376, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [3400/4200], Total Loss: 0.0410, Classification Loss: 0.0393, Inhibition Loss: 0.0017, Accuracy: 99.12%, Avg p_inhibited: 0.0015\n","Epoch [3/10], Step [3500/4200], Total Loss: 0.0369, Classification Loss: 0.0367, Inhibition Loss: 0.0002, Accuracy: 98.75%, Avg p_inhibited: 0.0002\n","Epoch [3/10], Step [3600/4200], Total Loss: 0.0294, Classification Loss: 0.0289, Inhibition Loss: 0.0005, Accuracy: 99.25%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [3700/4200], Total Loss: 0.0203, Classification Loss: 0.0199, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [3800/4200], Total Loss: 0.0303, Classification Loss: 0.0299, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0004\n","Epoch [3/10], Step [3900/4200], Total Loss: 0.0242, Classification Loss: 0.0236, Inhibition Loss: 0.0006, Accuracy: 99.62%, Avg p_inhibited: 0.0006\n","Epoch [3/10], Step [4000/4200], Total Loss: 0.0285, Classification Loss: 0.0282, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [3/10], Step [4100/4200], Total Loss: 0.0430, Classification Loss: 0.0423, Inhibition Loss: 0.0007, Accuracy: 99.12%, Avg p_inhibited: 0.0006\n","Epoch [3/10], Step [4200/4200], Total Loss: 0.0594, Classification Loss: 0.0590, Inhibition Loss: 0.0005, Accuracy: 98.50%, Avg p_inhibited: 0.0005\n","Epoch [4/10], Step [100/4200], Total Loss: 0.0355, Classification Loss: 0.0343, Inhibition Loss: 0.0011, Accuracy: 98.50%, Avg p_inhibited: 0.0008\n","Epoch [4/10], Step [200/4200], Total Loss: 0.0360, Classification Loss: 0.0330, Inhibition Loss: 0.0031, Accuracy: 99.25%, Avg p_inhibited: 0.0013\n","Epoch [4/10], Step [300/4200], Total Loss: 0.0420, Classification Loss: 0.0411, Inhibition Loss: 0.0010, Accuracy: 98.50%, Avg p_inhibited: 0.0009\n","Epoch [4/10], Step [400/4200], Total Loss: 0.0342, Classification Loss: 0.0338, Inhibition Loss: 0.0005, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [500/4200], Total Loss: 0.0202, Classification Loss: 0.0196, Inhibition Loss: 0.0006, Accuracy: 99.25%, Avg p_inhibited: 0.0006\n","Epoch [4/10], Step [600/4200], Total Loss: 0.0205, Classification Loss: 0.0201, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [700/4200], Total Loss: 0.0252, Classification Loss: 0.0250, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [800/4200], Total Loss: 0.0390, Classification Loss: 0.0388, Inhibition Loss: 0.0002, Accuracy: 98.62%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [900/4200], Total Loss: 0.0288, Classification Loss: 0.0287, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [4/10], Step [1000/4200], Total Loss: 0.0269, Classification Loss: 0.0265, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [1100/4200], Total Loss: 0.0272, Classification Loss: 0.0271, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [4/10], Step [1200/4200], Total Loss: 0.0219, Classification Loss: 0.0216, Inhibition Loss: 0.0003, Accuracy: 99.38%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [1300/4200], Total Loss: 0.0228, Classification Loss: 0.0224, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [1400/4200], Total Loss: 0.0281, Classification Loss: 0.0278, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [1500/4200], Total Loss: 0.0281, Classification Loss: 0.0279, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [1600/4200], Total Loss: 0.0298, Classification Loss: 0.0291, Inhibition Loss: 0.0007, Accuracy: 99.12%, Avg p_inhibited: 0.0006\n","Epoch [4/10], Step [1700/4200], Total Loss: 0.0413, Classification Loss: 0.0405, Inhibition Loss: 0.0008, Accuracy: 98.38%, Avg p_inhibited: 0.0007\n","Epoch [4/10], Step [1800/4200], Total Loss: 0.0309, Classification Loss: 0.0307, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [1900/4200], Total Loss: 0.0281, Classification Loss: 0.0278, Inhibition Loss: 0.0003, Accuracy: 99.38%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [2000/4200], Total Loss: 0.0352, Classification Loss: 0.0347, Inhibition Loss: 0.0005, Accuracy: 99.38%, Avg p_inhibited: 0.0005\n","Epoch [4/10], Step [2100/4200], Total Loss: 0.0261, Classification Loss: 0.0256, Inhibition Loss: 0.0004, Accuracy: 98.88%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [2200/4200], Total Loss: 0.0417, Classification Loss: 0.0415, Inhibition Loss: 0.0002, Accuracy: 98.50%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [2300/4200], Total Loss: 0.0323, Classification Loss: 0.0316, Inhibition Loss: 0.0007, Accuracy: 99.00%, Avg p_inhibited: 0.0006\n","Epoch [4/10], Step [2400/4200], Total Loss: 0.0314, Classification Loss: 0.0312, Inhibition Loss: 0.0002, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [2500/4200], Total Loss: 0.0355, Classification Loss: 0.0353, Inhibition Loss: 0.0002, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [2600/4200], Total Loss: 0.0340, Classification Loss: 0.0339, Inhibition Loss: 0.0002, Accuracy: 99.50%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [2700/4200], Total Loss: 0.0287, Classification Loss: 0.0281, Inhibition Loss: 0.0006, Accuracy: 99.00%, Avg p_inhibited: 0.0005\n","Epoch [4/10], Step [2800/4200], Total Loss: 0.0234, Classification Loss: 0.0229, Inhibition Loss: 0.0004, Accuracy: 99.62%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [2900/4200], Total Loss: 0.0239, Classification Loss: 0.0235, Inhibition Loss: 0.0004, Accuracy: 98.88%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [3000/4200], Total Loss: 0.0400, Classification Loss: 0.0399, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [4/10], Step [3100/4200], Total Loss: 0.0143, Classification Loss: 0.0142, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [4/10], Step [3200/4200], Total Loss: 0.0409, Classification Loss: 0.0407, Inhibition Loss: 0.0002, Accuracy: 98.50%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [3300/4200], Total Loss: 0.0181, Classification Loss: 0.0176, Inhibition Loss: 0.0004, Accuracy: 99.50%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [3400/4200], Total Loss: 0.0275, Classification Loss: 0.0270, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [3500/4200], Total Loss: 0.0355, Classification Loss: 0.0354, Inhibition Loss: 0.0001, Accuracy: 98.75%, Avg p_inhibited: 0.0001\n","Epoch [4/10], Step [3600/4200], Total Loss: 0.0422, Classification Loss: 0.0421, Inhibition Loss: 0.0002, Accuracy: 99.12%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [3700/4200], Total Loss: 0.0187, Classification Loss: 0.0184, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [3800/4200], Total Loss: 0.0157, Classification Loss: 0.0155, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [4/10], Step [3900/4200], Total Loss: 0.0405, Classification Loss: 0.0401, Inhibition Loss: 0.0004, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [4000/4200], Total Loss: 0.0491, Classification Loss: 0.0487, Inhibition Loss: 0.0004, Accuracy: 99.00%, Avg p_inhibited: 0.0004\n","Epoch [4/10], Step [4100/4200], Total Loss: 0.0184, Classification Loss: 0.0180, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [4/10], Step [4200/4200], Total Loss: 0.0422, Classification Loss: 0.0419, Inhibition Loss: 0.0002, Accuracy: 98.88%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [100/4200], Total Loss: 0.0221, Classification Loss: 0.0213, Inhibition Loss: 0.0008, Accuracy: 99.12%, Avg p_inhibited: 0.0007\n","Epoch [5/10], Step [200/4200], Total Loss: 0.0356, Classification Loss: 0.0348, Inhibition Loss: 0.0008, Accuracy: 99.12%, Avg p_inhibited: 0.0007\n","Epoch [5/10], Step [300/4200], Total Loss: 0.0286, Classification Loss: 0.0281, Inhibition Loss: 0.0004, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [400/4200], Total Loss: 0.0392, Classification Loss: 0.0387, Inhibition Loss: 0.0005, Accuracy: 98.25%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [500/4200], Total Loss: 0.0161, Classification Loss: 0.0157, Inhibition Loss: 0.0003, Accuracy: 99.62%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [600/4200], Total Loss: 0.0361, Classification Loss: 0.0359, Inhibition Loss: 0.0002, Accuracy: 99.50%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [700/4200], Total Loss: 0.0348, Classification Loss: 0.0345, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [800/4200], Total Loss: 0.0329, Classification Loss: 0.0326, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [900/4200], Total Loss: 0.0233, Classification Loss: 0.0232, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [1000/4200], Total Loss: 0.0126, Classification Loss: 0.0125, Inhibition Loss: 0.0001, Accuracy: 99.88%, Avg p_inhibited: 0.0001\n","Epoch [5/10], Step [1100/4200], Total Loss: 0.0347, Classification Loss: 0.0345, Inhibition Loss: 0.0002, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [1200/4200], Total Loss: 0.0238, Classification Loss: 0.0235, Inhibition Loss: 0.0004, Accuracy: 99.25%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [1300/4200], Total Loss: 0.0240, Classification Loss: 0.0239, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [5/10], Step [1400/4200], Total Loss: 0.0261, Classification Loss: 0.0259, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [1500/4200], Total Loss: 0.0266, Classification Loss: 0.0251, Inhibition Loss: 0.0015, Accuracy: 99.12%, Avg p_inhibited: 0.0013\n","Epoch [5/10], Step [1600/4200], Total Loss: 0.0347, Classification Loss: 0.0337, Inhibition Loss: 0.0010, Accuracy: 99.25%, Avg p_inhibited: 0.0009\n","Epoch [5/10], Step [1700/4200], Total Loss: 0.0294, Classification Loss: 0.0289, Inhibition Loss: 0.0005, Accuracy: 99.00%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [1800/4200], Total Loss: 0.0390, Classification Loss: 0.0389, Inhibition Loss: 0.0001, Accuracy: 98.88%, Avg p_inhibited: 0.0001\n","Epoch [5/10], Step [1900/4200], Total Loss: 0.0256, Classification Loss: 0.0251, Inhibition Loss: 0.0005, Accuracy: 99.25%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [2000/4200], Total Loss: 0.0362, Classification Loss: 0.0357, Inhibition Loss: 0.0005, Accuracy: 98.75%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [2100/4200], Total Loss: 0.0353, Classification Loss: 0.0349, Inhibition Loss: 0.0004, Accuracy: 98.62%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [2200/4200], Total Loss: 0.0406, Classification Loss: 0.0398, Inhibition Loss: 0.0008, Accuracy: 98.50%, Avg p_inhibited: 0.0007\n","Epoch [5/10], Step [2300/4200], Total Loss: 0.0231, Classification Loss: 0.0229, Inhibition Loss: 0.0002, Accuracy: 99.12%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [2400/4200], Total Loss: 0.0227, Classification Loss: 0.0222, Inhibition Loss: 0.0005, Accuracy: 99.62%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [2500/4200], Total Loss: 0.0282, Classification Loss: 0.0279, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [2600/4200], Total Loss: 0.0254, Classification Loss: 0.0248, Inhibition Loss: 0.0005, Accuracy: 99.00%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [2700/4200], Total Loss: 0.0160, Classification Loss: 0.0151, Inhibition Loss: 0.0009, Accuracy: 99.62%, Avg p_inhibited: 0.0007\n","Epoch [5/10], Step [2800/4200], Total Loss: 0.0417, Classification Loss: 0.0402, Inhibition Loss: 0.0015, Accuracy: 98.50%, Avg p_inhibited: 0.0010\n","Epoch [5/10], Step [2900/4200], Total Loss: 0.0174, Classification Loss: 0.0171, Inhibition Loss: 0.0002, Accuracy: 99.75%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [3000/4200], Total Loss: 0.0233, Classification Loss: 0.0230, Inhibition Loss: 0.0004, Accuracy: 99.12%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [3100/4200], Total Loss: 0.0135, Classification Loss: 0.0117, Inhibition Loss: 0.0018, Accuracy: 99.75%, Avg p_inhibited: 0.0011\n","Epoch [5/10], Step [3200/4200], Total Loss: 0.0326, Classification Loss: 0.0319, Inhibition Loss: 0.0007, Accuracy: 99.12%, Avg p_inhibited: 0.0007\n","Epoch [5/10], Step [3300/4200], Total Loss: 0.0196, Classification Loss: 0.0194, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [5/10], Step [3400/4200], Total Loss: 0.0276, Classification Loss: 0.0273, Inhibition Loss: 0.0003, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [3500/4200], Total Loss: 0.0237, Classification Loss: 0.0233, Inhibition Loss: 0.0004, Accuracy: 99.38%, Avg p_inhibited: 0.0004\n","Epoch [5/10], Step [3600/4200], Total Loss: 0.0245, Classification Loss: 0.0242, Inhibition Loss: 0.0003, Accuracy: 99.38%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [3700/4200], Total Loss: 0.0268, Classification Loss: 0.0265, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [5/10], Step [3800/4200], Total Loss: 0.0286, Classification Loss: 0.0285, Inhibition Loss: 0.0001, Accuracy: 98.88%, Avg p_inhibited: 0.0001\n","Epoch [5/10], Step [3900/4200], Total Loss: 0.0234, Classification Loss: 0.0233, Inhibition Loss: 0.0000, Accuracy: 99.25%, Avg p_inhibited: 0.0000\n","Epoch [5/10], Step [4000/4200], Total Loss: 0.0238, Classification Loss: 0.0236, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [5/10], Step [4100/4200], Total Loss: 0.0421, Classification Loss: 0.0416, Inhibition Loss: 0.0006, Accuracy: 98.12%, Avg p_inhibited: 0.0005\n","Epoch [5/10], Step [4200/4200], Total Loss: 0.0176, Classification Loss: 0.0174, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [100/4200], Total Loss: 0.0470, Classification Loss: 0.0468, Inhibition Loss: 0.0002, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [200/4200], Total Loss: 0.0233, Classification Loss: 0.0230, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0003\n","Epoch [6/10], Step [300/4200], Total Loss: 0.0266, Classification Loss: 0.0265, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [400/4200], Total Loss: 0.0484, Classification Loss: 0.0483, Inhibition Loss: 0.0001, Accuracy: 98.75%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [500/4200], Total Loss: 0.0119, Classification Loss: 0.0115, Inhibition Loss: 0.0004, Accuracy: 99.75%, Avg p_inhibited: 0.0004\n","Epoch [6/10], Step [600/4200], Total Loss: 0.0219, Classification Loss: 0.0218, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [700/4200], Total Loss: 0.0276, Classification Loss: 0.0274, Inhibition Loss: 0.0002, Accuracy: 98.75%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [800/4200], Total Loss: 0.0151, Classification Loss: 0.0149, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [900/4200], Total Loss: 0.0195, Classification Loss: 0.0193, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [1000/4200], Total Loss: 0.0191, Classification Loss: 0.0189, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [1100/4200], Total Loss: 0.0220, Classification Loss: 0.0217, Inhibition Loss: 0.0004, Accuracy: 99.50%, Avg p_inhibited: 0.0004\n","Epoch [6/10], Step [1200/4200], Total Loss: 0.0406, Classification Loss: 0.0404, Inhibition Loss: 0.0002, Accuracy: 98.75%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [1300/4200], Total Loss: 0.0344, Classification Loss: 0.0323, Inhibition Loss: 0.0021, Accuracy: 98.88%, Avg p_inhibited: 0.0011\n","Epoch [6/10], Step [1400/4200], Total Loss: 0.0204, Classification Loss: 0.0199, Inhibition Loss: 0.0005, Accuracy: 99.50%, Avg p_inhibited: 0.0005\n","Epoch [6/10], Step [1500/4200], Total Loss: 0.0398, Classification Loss: 0.0391, Inhibition Loss: 0.0006, Accuracy: 98.62%, Avg p_inhibited: 0.0006\n","Epoch [6/10], Step [1600/4200], Total Loss: 0.0170, Classification Loss: 0.0168, Inhibition Loss: 0.0002, Accuracy: 99.50%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [1700/4200], Total Loss: 0.0244, Classification Loss: 0.0211, Inhibition Loss: 0.0033, Accuracy: 99.25%, Avg p_inhibited: 0.0014\n","Epoch [6/10], Step [1800/4200], Total Loss: 0.0305, Classification Loss: 0.0272, Inhibition Loss: 0.0033, Accuracy: 99.12%, Avg p_inhibited: 0.0013\n","Epoch [6/10], Step [1900/4200], Total Loss: 0.0323, Classification Loss: 0.0322, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2000/4200], Total Loss: 0.0343, Classification Loss: 0.0340, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [6/10], Step [2100/4200], Total Loss: 0.0340, Classification Loss: 0.0339, Inhibition Loss: 0.0001, Accuracy: 98.62%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2200/4200], Total Loss: 0.0188, Classification Loss: 0.0187, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [2300/4200], Total Loss: 0.0157, Classification Loss: 0.0156, Inhibition Loss: 0.0001, Accuracy: 99.62%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2400/4200], Total Loss: 0.0159, Classification Loss: 0.0156, Inhibition Loss: 0.0004, Accuracy: 99.38%, Avg p_inhibited: 0.0003\n","Epoch [6/10], Step [2500/4200], Total Loss: 0.0209, Classification Loss: 0.0208, Inhibition Loss: 0.0001, Accuracy: 99.38%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2600/4200], Total Loss: 0.0284, Classification Loss: 0.0282, Inhibition Loss: 0.0001, Accuracy: 98.88%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2700/4200], Total Loss: 0.0216, Classification Loss: 0.0215, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [2800/4200], Total Loss: 0.0247, Classification Loss: 0.0242, Inhibition Loss: 0.0006, Accuracy: 99.12%, Avg p_inhibited: 0.0005\n","Epoch [6/10], Step [2900/4200], Total Loss: 0.0431, Classification Loss: 0.0428, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [6/10], Step [3000/4200], Total Loss: 0.0203, Classification Loss: 0.0201, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [3100/4200], Total Loss: 0.0282, Classification Loss: 0.0253, Inhibition Loss: 0.0029, Accuracy: 99.00%, Avg p_inhibited: 0.0018\n","Epoch [6/10], Step [3200/4200], Total Loss: 0.0179, Classification Loss: 0.0178, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [3300/4200], Total Loss: 0.0399, Classification Loss: 0.0394, Inhibition Loss: 0.0004, Accuracy: 98.88%, Avg p_inhibited: 0.0004\n","Epoch [6/10], Step [3400/4200], Total Loss: 0.0278, Classification Loss: 0.0273, Inhibition Loss: 0.0005, Accuracy: 99.25%, Avg p_inhibited: 0.0005\n","Epoch [6/10], Step [3500/4200], Total Loss: 0.0169, Classification Loss: 0.0164, Inhibition Loss: 0.0005, Accuracy: 99.38%, Avg p_inhibited: 0.0004\n","Epoch [6/10], Step [3600/4200], Total Loss: 0.0253, Classification Loss: 0.0251, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0002\n","Epoch [6/10], Step [3700/4200], Total Loss: 0.0154, Classification Loss: 0.0149, Inhibition Loss: 0.0005, Accuracy: 99.75%, Avg p_inhibited: 0.0005\n","Epoch [6/10], Step [3800/4200], Total Loss: 0.0264, Classification Loss: 0.0262, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [3900/4200], Total Loss: 0.0255, Classification Loss: 0.0253, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [6/10], Step [4000/4200], Total Loss: 0.0433, Classification Loss: 0.0431, Inhibition Loss: 0.0003, Accuracy: 98.88%, Avg p_inhibited: 0.0003\n","Epoch [6/10], Step [4100/4200], Total Loss: 0.0179, Classification Loss: 0.0175, Inhibition Loss: 0.0004, Accuracy: 99.50%, Avg p_inhibited: 0.0004\n","Epoch [6/10], Step [4200/4200], Total Loss: 0.0226, Classification Loss: 0.0222, Inhibition Loss: 0.0004, Accuracy: 99.38%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [100/4200], Total Loss: 0.0223, Classification Loss: 0.0222, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [200/4200], Total Loss: 0.0182, Classification Loss: 0.0180, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [300/4200], Total Loss: 0.0390, Classification Loss: 0.0387, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [400/4200], Total Loss: 0.0165, Classification Loss: 0.0163, Inhibition Loss: 0.0001, Accuracy: 99.62%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [500/4200], Total Loss: 0.0181, Classification Loss: 0.0181, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [600/4200], Total Loss: 0.0280, Classification Loss: 0.0270, Inhibition Loss: 0.0009, Accuracy: 99.12%, Avg p_inhibited: 0.0008\n","Epoch [7/10], Step [700/4200], Total Loss: 0.0169, Classification Loss: 0.0169, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [800/4200], Total Loss: 0.0397, Classification Loss: 0.0396, Inhibition Loss: 0.0002, Accuracy: 98.75%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [900/4200], Total Loss: 0.0317, Classification Loss: 0.0279, Inhibition Loss: 0.0037, Accuracy: 99.00%, Avg p_inhibited: 0.0017\n","Epoch [7/10], Step [1000/4200], Total Loss: 0.0413, Classification Loss: 0.0409, Inhibition Loss: 0.0004, Accuracy: 99.00%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [1100/4200], Total Loss: 0.0204, Classification Loss: 0.0202, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [1200/4200], Total Loss: 0.0334, Classification Loss: 0.0332, Inhibition Loss: 0.0002, Accuracy: 98.88%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [1300/4200], Total Loss: 0.0408, Classification Loss: 0.0404, Inhibition Loss: 0.0005, Accuracy: 98.75%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [1400/4200], Total Loss: 0.0302, Classification Loss: 0.0290, Inhibition Loss: 0.0012, Accuracy: 99.00%, Avg p_inhibited: 0.0009\n","Epoch [7/10], Step [1500/4200], Total Loss: 0.0154, Classification Loss: 0.0153, Inhibition Loss: 0.0001, Accuracy: 99.75%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [1600/4200], Total Loss: 0.0319, Classification Loss: 0.0316, Inhibition Loss: 0.0003, Accuracy: 99.00%, Avg p_inhibited: 0.0003\n","Epoch [7/10], Step [1700/4200], Total Loss: 0.0291, Classification Loss: 0.0282, Inhibition Loss: 0.0009, Accuracy: 99.25%, Avg p_inhibited: 0.0007\n","Epoch [7/10], Step [1800/4200], Total Loss: 0.0167, Classification Loss: 0.0156, Inhibition Loss: 0.0010, Accuracy: 99.62%, Avg p_inhibited: 0.0008\n","Epoch [7/10], Step [1900/4200], Total Loss: 0.0282, Classification Loss: 0.0281, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [2000/4200], Total Loss: 0.0228, Classification Loss: 0.0226, Inhibition Loss: 0.0002, Accuracy: 99.12%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [2100/4200], Total Loss: 0.0154, Classification Loss: 0.0153, Inhibition Loss: 0.0001, Accuracy: 99.75%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [2200/4200], Total Loss: 0.0267, Classification Loss: 0.0266, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [2300/4200], Total Loss: 0.0148, Classification Loss: 0.0146, Inhibition Loss: 0.0001, Accuracy: 99.62%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [2400/4200], Total Loss: 0.0434, Classification Loss: 0.0432, Inhibition Loss: 0.0001, Accuracy: 98.62%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [2500/4200], Total Loss: 0.0254, Classification Loss: 0.0244, Inhibition Loss: 0.0010, Accuracy: 99.38%, Avg p_inhibited: 0.0008\n","Epoch [7/10], Step [2600/4200], Total Loss: 0.0182, Classification Loss: 0.0177, Inhibition Loss: 0.0005, Accuracy: 99.38%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [2700/4200], Total Loss: 0.0251, Classification Loss: 0.0248, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [2800/4200], Total Loss: 0.0226, Classification Loss: 0.0224, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [2900/4200], Total Loss: 0.0249, Classification Loss: 0.0249, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [3000/4200], Total Loss: 0.0224, Classification Loss: 0.0223, Inhibition Loss: 0.0001, Accuracy: 99.50%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [3100/4200], Total Loss: 0.0246, Classification Loss: 0.0245, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [3200/4200], Total Loss: 0.0171, Classification Loss: 0.0167, Inhibition Loss: 0.0004, Accuracy: 99.50%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [3300/4200], Total Loss: 0.0271, Classification Loss: 0.0256, Inhibition Loss: 0.0015, Accuracy: 99.00%, Avg p_inhibited: 0.0009\n","Epoch [7/10], Step [3400/4200], Total Loss: 0.0351, Classification Loss: 0.0319, Inhibition Loss: 0.0032, Accuracy: 99.25%, Avg p_inhibited: 0.0017\n","Epoch [7/10], Step [3500/4200], Total Loss: 0.0481, Classification Loss: 0.0448, Inhibition Loss: 0.0033, Accuracy: 99.00%, Avg p_inhibited: 0.0014\n","Epoch [7/10], Step [3600/4200], Total Loss: 0.0156, Classification Loss: 0.0154, Inhibition Loss: 0.0002, Accuracy: 99.75%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [3700/4200], Total Loss: 0.0114, Classification Loss: 0.0110, Inhibition Loss: 0.0004, Accuracy: 99.88%, Avg p_inhibited: 0.0004\n","Epoch [7/10], Step [3800/4200], Total Loss: 0.0349, Classification Loss: 0.0348, Inhibition Loss: 0.0001, Accuracy: 98.75%, Avg p_inhibited: 0.0001\n","Epoch [7/10], Step [3900/4200], Total Loss: 0.0181, Classification Loss: 0.0179, Inhibition Loss: 0.0002, Accuracy: 99.50%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [4000/4200], Total Loss: 0.0219, Classification Loss: 0.0215, Inhibition Loss: 0.0003, Accuracy: 99.12%, Avg p_inhibited: 0.0003\n","Epoch [7/10], Step [4100/4200], Total Loss: 0.0176, Classification Loss: 0.0174, Inhibition Loss: 0.0002, Accuracy: 99.62%, Avg p_inhibited: 0.0002\n","Epoch [7/10], Step [4200/4200], Total Loss: 0.0225, Classification Loss: 0.0215, Inhibition Loss: 0.0010, Accuracy: 99.00%, Avg p_inhibited: 0.0008\n","Epoch [8/10], Step [100/4200], Total Loss: 0.0247, Classification Loss: 0.0244, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [8/10], Step [200/4200], Total Loss: 0.0233, Classification Loss: 0.0229, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [8/10], Step [300/4200], Total Loss: 0.0220, Classification Loss: 0.0218, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [400/4200], Total Loss: 0.0307, Classification Loss: 0.0302, Inhibition Loss: 0.0005, Accuracy: 99.00%, Avg p_inhibited: 0.0004\n","Epoch [8/10], Step [500/4200], Total Loss: 0.0214, Classification Loss: 0.0210, Inhibition Loss: 0.0003, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [8/10], Step [600/4200], Total Loss: 0.0225, Classification Loss: 0.0224, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [700/4200], Total Loss: 0.0133, Classification Loss: 0.0130, Inhibition Loss: 0.0003, Accuracy: 99.62%, Avg p_inhibited: 0.0003\n","Epoch [8/10], Step [800/4200], Total Loss: 0.0227, Classification Loss: 0.0193, Inhibition Loss: 0.0034, Accuracy: 99.50%, Avg p_inhibited: 0.0013\n","Epoch [8/10], Step [900/4200], Total Loss: 0.0259, Classification Loss: 0.0258, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [1000/4200], Total Loss: 0.0233, Classification Loss: 0.0231, Inhibition Loss: 0.0002, Accuracy: 99.50%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [1100/4200], Total Loss: 0.0263, Classification Loss: 0.0262, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [1200/4200], Total Loss: 0.0280, Classification Loss: 0.0275, Inhibition Loss: 0.0005, Accuracy: 99.12%, Avg p_inhibited: 0.0005\n","Epoch [8/10], Step [1300/4200], Total Loss: 0.0147, Classification Loss: 0.0146, Inhibition Loss: 0.0001, Accuracy: 99.75%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [1400/4200], Total Loss: 0.0316, Classification Loss: 0.0291, Inhibition Loss: 0.0025, Accuracy: 99.12%, Avg p_inhibited: 0.0015\n","Epoch [8/10], Step [1500/4200], Total Loss: 0.0265, Classification Loss: 0.0263, Inhibition Loss: 0.0001, Accuracy: 99.38%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [1600/4200], Total Loss: 0.0223, Classification Loss: 0.0220, Inhibition Loss: 0.0003, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [1700/4200], Total Loss: 0.0144, Classification Loss: 0.0143, Inhibition Loss: 0.0001, Accuracy: 99.62%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [1800/4200], Total Loss: 0.0190, Classification Loss: 0.0190, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [1900/4200], Total Loss: 0.0148, Classification Loss: 0.0147, Inhibition Loss: 0.0001, Accuracy: 99.62%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [2000/4200], Total Loss: 0.0176, Classification Loss: 0.0169, Inhibition Loss: 0.0006, Accuracy: 99.75%, Avg p_inhibited: 0.0005\n","Epoch [8/10], Step [2100/4200], Total Loss: 0.0148, Classification Loss: 0.0147, Inhibition Loss: 0.0001, Accuracy: 99.75%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [2200/4200], Total Loss: 0.0215, Classification Loss: 0.0207, Inhibition Loss: 0.0007, Accuracy: 99.62%, Avg p_inhibited: 0.0006\n","Epoch [8/10], Step [2300/4200], Total Loss: 0.0236, Classification Loss: 0.0235, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [2400/4200], Total Loss: 0.0274, Classification Loss: 0.0273, Inhibition Loss: 0.0001, Accuracy: 99.00%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [2500/4200], Total Loss: 0.0259, Classification Loss: 0.0247, Inhibition Loss: 0.0012, Accuracy: 99.38%, Avg p_inhibited: 0.0008\n","Epoch [8/10], Step [2600/4200], Total Loss: 0.0288, Classification Loss: 0.0286, Inhibition Loss: 0.0002, Accuracy: 99.25%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [2700/4200], Total Loss: 0.0195, Classification Loss: 0.0191, Inhibition Loss: 0.0004, Accuracy: 99.50%, Avg p_inhibited: 0.0003\n","Epoch [8/10], Step [2800/4200], Total Loss: 0.0300, Classification Loss: 0.0290, Inhibition Loss: 0.0009, Accuracy: 99.00%, Avg p_inhibited: 0.0008\n","Epoch [8/10], Step [2900/4200], Total Loss: 0.0155, Classification Loss: 0.0153, Inhibition Loss: 0.0002, Accuracy: 99.38%, Avg p_inhibited: 0.0002\n","Epoch [8/10], Step [3000/4200], Total Loss: 0.0416, Classification Loss: 0.0404, Inhibition Loss: 0.0012, Accuracy: 99.25%, Avg p_inhibited: 0.0008\n","Epoch [8/10], Step [3100/4200], Total Loss: 0.0231, Classification Loss: 0.0230, Inhibition Loss: 0.0001, Accuracy: 99.12%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [3200/4200], Total Loss: 0.0435, Classification Loss: 0.0434, Inhibition Loss: 0.0001, Accuracy: 98.50%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [3300/4200], Total Loss: 0.0116, Classification Loss: 0.0115, Inhibition Loss: 0.0001, Accuracy: 99.75%, Avg p_inhibited: 0.0001\n","Epoch [8/10], Step [3400/4200], Total Loss: 0.0273, Classification Loss: 0.0262, Inhibition Loss: 0.0011, Accuracy: 99.25%, Avg p_inhibited: 0.0009\n","Epoch [8/10], Step [3500/4200], Total Loss: 0.0261, Classification Loss: 0.0260, Inhibition Loss: 0.0001, Accuracy: 99.25%, Avg p_inhibited: 0.0001\n"]}],"source":["# Import any required modules\n","import torch.nn.functional as F\n","\n","# Training Loop\n","puppeteer_net = PuppeteerNet()\n","\n","# Freeze CNN parameters\n","for param in conv_model.parameters():\n","    param.requires_grad = False\n","\n","if torch.cuda.is_available():\n","    conv_model = conv_model.cuda()\n","    puppeteer_net = puppeteer_net.cuda()\n","    criterion = criterion.cuda()\n","\n","# Define optimizer for the Puppeteer MLP\n","optimizer = optim.Adam(puppeteer_net.parameters(), lr=0.001)\n","\n","# Training loop parameters\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    puppeteer_net.train()\n","    running_total_loss = 0.0\n","    running_classification_loss = 0.0\n","    running_inhibition_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    total_p_inhibited = 0.0  # Sum of probabilities of inhibited digits\n","\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.unsqueeze(1)\n","        if torch.cuda.is_available():\n","            images = images.cuda()\n","            labels = labels.cuda()\n","        \n","        batch_size = images.size(0)\n","        \n","        # Convert labels to LongTensor and create random tensor with matching dtype\n","        labels = labels.long()\n","        random_offset = torch.randint(1, 9, (batch_size,), dtype=torch.long, device=labels.device)\n","        inhibited_digits = (labels + random_offset) % 10\n","        \n","        # Convert inhibited digits to one-hot vectors\n","        inhibited_digits_one_hot = torch.zeros(batch_size, 10).to(images.device)\n","        inhibited_digits_one_hot.scatter_(1, inhibited_digits.view(-1, 1), 1)\n","        \n","        # Get adjustments from Puppeteer MLP\n","        adjustments = puppeteer_net(inhibited_digits_one_hot)\n","        \n","        # Verify that adjustments is a dictionary\n","        if not isinstance(adjustments, dict):\n","            raise TypeError(\"Adjustments must be a dictionary.\")\n","        \n","        # Forward pass through the CNN with adjustments\n","        output = conv_model(images, adjustments=adjustments)\n","        \n","        # Compute softmax probabilities\n","        output_probs = F.softmax(output, dim=1)\n","        \n","        # Probability of predicting the inhibited digit\n","        p_inhibited = output_probs[range(batch_size), inhibited_digits]\n","        \n","        # Inhibition loss\n","        epsilon = 1e-6  # Small value to prevent log(0)\n","        inhibition_loss = -torch.log(1 - p_inhibited + epsilon)\n","        inhibition_loss = torch.mean(inhibition_loss)\n","        \n","        # Classification loss\n","        classification_loss = criterion(output, labels)\n","        \n","        # Total loss\n","        total_loss = classification_loss + inhibition_loss\n","        \n","        # Backpropagation\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        optimizer.step()\n","        \n","        running_total_loss += total_loss.item()\n","        running_classification_loss += classification_loss.item()\n","        running_inhibition_loss += inhibition_loss.item()\n","        \n","        # Calculate accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        total_samples += labels.size(0)\n","        total_correct += (predicted == labels).sum().item()\n","        \n","        # Sum probabilities of inhibited digits\n","        total_p_inhibited += p_inhibited.sum().item()\n","        \n","        # Print training progress\n","        if (i+1) % 100 == 0:\n","            avg_accuracy = 100.0 * total_correct / total_samples\n","            avg_p_inhibited = total_p_inhibited / total_samples\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n","                  f'Total Loss: {running_total_loss/100:.4f}, '\n","                  f'Classification Loss: {running_classification_loss/100:.4f}, '\n","                  f'Inhibition Loss: {running_inhibition_loss/100:.4f}, '\n","                  f'Accuracy: {avg_accuracy:.2f}%, '\n","                  f'Avg p_inhibited: {avg_p_inhibited:.4f}')\n","            running_total_loss = 0.0\n","            running_classification_loss = 0.0\n","            running_inhibition_loss = 0.0\n","            total_correct = 0\n","            total_samples = 0\n","            total_p_inhibited = 0.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"}},"nbformat":4,"nbformat_minor":1}